{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Image\n",
    "import os\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import ast  # To safely evaluate JSON-like strings\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_click_point(\n",
    "    img_path: str,\n",
    "    naive_x: float, naive_y: float,\n",
    "    uigraph_x: float, uigraph_y: float,\n",
    "    bbox: list,\n",
    "    instruction: str,\n",
    "    gap_between_text_and_image: int = 15\n",
    ") -> bytes:\n",
    "    \"\"\"\n",
    "    Draws a white instruction banner at the top, leaves a gap,\n",
    "    then the original image annotated with:\n",
    "      - Green rectangle for ground-truth bbox\n",
    "      - Blue dot for Naive prediction (naive_x, naive_y)\n",
    "      - Red dot for UI-Graph prediction (uigraph_x, uigraph_y)\n",
    "\n",
    "    Returns image bytes in PNG format.\n",
    "    \"\"\"\n",
    "    # 1) Load original image\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    width, height = image.size\n",
    "\n",
    "    # 2) Load a font that supports Chinese (fallback to default)\n",
    "    try:\n",
    "        font = ImageFont.truetype(\n",
    "            \"/usr/share/fonts/truetype/noto/NotoSansSC-Regular.otf\", size=25\n",
    "        )\n",
    "    except IOError:\n",
    "        font = ImageFont.load_default(size=25)\n",
    "\n",
    "    # 3) Measure instruction text size\n",
    "    tmp_draw = ImageDraw.Draw(image)\n",
    "    text_bbox = tmp_draw.textbbox((0, 0), 'Instruction : ' + instruction, font=font)\n",
    "    text_w = text_bbox[2] - text_bbox[0]\n",
    "    text_h = text_bbox[3] - text_bbox[1]\n",
    "\n",
    "    # 4) Compute padding for banner and gap\n",
    "    top_margin = 5\n",
    "    bottom_margin = gap_between_text_and_image\n",
    "    padding_top = text_h + top_margin + bottom_margin\n",
    "\n",
    "    # 5) Create new canvas and paste original image\n",
    "    new_img = Image.new(\"RGB\", (width, height + padding_top), \"white\")\n",
    "    new_img.paste(image, (0, padding_top))\n",
    "\n",
    "    draw = ImageDraw.Draw(new_img)\n",
    "\n",
    "    # 6) Draw instruction centered\n",
    "    x_text = (width - text_w) / 2\n",
    "    y_text = top_margin\n",
    "    draw.text((x_text, y_text), instruction, fill=\"black\", font=font)\n",
    "\n",
    "    # 7) Helper to convert relative coords to absolute\n",
    "    def to_abs(rx, ry):\n",
    "        return rx * width, padding_top + ry * height\n",
    "\n",
    "    nx, ny = to_abs(naive_x, naive_y)\n",
    "    ux, uy = to_abs(uigraph_x, uigraph_y)\n",
    "    bx0, by0 = to_abs(bbox[0], bbox[1])\n",
    "    bx1, by1 = to_abs(bbox[2], bbox[3])\n",
    "\n",
    "    # 8) Draw bounding box and dots\n",
    "    draw.rectangle([bx0, by0, bx1, by1], outline=\"green\", width=3)\n",
    "    r = 8\n",
    "    # Blue dot = Naive prediction\n",
    "    br = 9\n",
    "    draw.ellipse([nx - br, ny - br, nx + br, ny + br], fill=\"blue\")\n",
    "    # Red dot = UI-Graph prediction\n",
    "    draw.ellipse([ux - r, uy - r, ux + r, uy + r], fill=\"red\")\n",
    "\n",
    "    # 9) Export to PNG bytes\n",
    "    with io.BytesIO() as out:\n",
    "        new_img.save(out, format=\"PNG\")\n",
    "        return out.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON file\n",
    "naive_json_path = \"/home/syc/intern/wanshan/Qwen2-VL/agent_tasks/ScreenSpot/naive/screenspot_naive-mobile.json\"\n",
    "\n",
    "with open(naive_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    naive_data = json.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "uigraph_json_path = \"/home/syc/intern/wanshan/Qwen2-VL/agent_tasks/ScreenSpot/uigraph_prunelayer_20-04-30/screenspot_uigraph_qwen2vl-7b_dropratio-0.2_mobile-prune-layer_20.json\"\n",
    "with open(uigraph_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    uigraph_data = json.load(f)\n",
    "# Filter instances where Ele_match is False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'img_path': '/data/data1/syc/intern/wanshan/datasets/ScreenSpot/screenspot_imgs/mobile_1ca5b944-293a-46a1-af95-eb35bc8a0b2a.png',\n",
       " 'text': 'check the weather',\n",
       " 'bbox': [0.09449152542372881,\n",
       "  0.0475609756097561,\n",
       "  0.34915254237288135,\n",
       "  0.4091463414634146],\n",
       " 'pred': [0.2, 0.17],\n",
       " 'matched': True,\n",
       " 'response': '{\"action_type\": 4, \"click_point\": (0.20,0.17)}\\n',\n",
       " 'type': 'icon',\n",
       " 'source': 'ios'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uigraph_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.09449152542372881,\n",
       " 0.0475609756097561,\n",
       " 0.34915254237288135,\n",
       " 0.4091463414634146]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_data[0]['bbox']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(502, 502)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(naive_data), len(uigraph_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mismatched_idxs  = []\n",
    "for sample_idx, sample in enumerate(naive_data):\n",
    "    if 'matched' in sample and 'matched' in uigraph_data[sample_idx]:\n",
    "        if sample['matched'] == False and uigraph_data[sample_idx]['matched'] == True:\n",
    "            mismatched_idxs.append(sample_idx)\n",
    "len(mismatched_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample_idx in mismatched_idxs:\n",
    "    if naive_data[sample_idx]['img_path'] != uigraph_data[sample_idx]['img_path']:\n",
    "        print('mismatched image_path', naive_data[sample_idx]['img_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5100aeaeaca46c2884e3a2806004bce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch \n",
    "from Qwen2VL_uigraph.model_prunelayer import Qwen2VLForConditionalGeneration\n",
    "\n",
    "device = 'cuda'\n",
    "prune_layer = 6 \n",
    "model_path = \"/home/syc/intern/wanshan/Qwen2VL-Resampler-Finetune/output/resampler_7b_retain_ratio_1\"\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=device,\n",
    "    prune_layer=prune_layer,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from Qwen2VL_uigraph.model_prunelayer import Qwen2VLProcessor\n",
    "\n",
    "\n",
    "\n",
    "# min_pixel = 1344*28*28\n",
    "max_pixel = 1680*28*28\n",
    "# 1. Screenshot -> Graph\n",
    "uigraph_train = True  # Enable ui graph during training\n",
    "uigraph_test = True  # Enable ui graph during inference\n",
    "uigraph_diff = 1  # Pixel difference used for constructing ui graph\n",
    "uigraph_rand = False  # Enable random graph construction\n",
    "# 2. Graph -> Mask\n",
    "uimask_pre = True  # Prebuild patch selection mask in the preprocessor (not in model layers) for efficiency\n",
    "uimask_ratio = 0.2 # Specify the percentage of patch tokens to skip per component\n",
    "uimask_rand = False  # Enable random token selection instead of uniform selection\n",
    "\n",
    "\n",
    "processor = Qwen2VLProcessor.from_pretrained(\n",
    "    model_path,\n",
    "    # min_pixels= min_pixel,\n",
    "    max_pixels = max_pixel,\n",
    "    uigraph_train=uigraph_train,\n",
    "    uigraph_test=uigraph_test,\n",
    "    uigraph_diff=uigraph_diff,\n",
    "    uigraph_rand=uigraph_rand,\n",
    "    uimask_pre=True,\n",
    "    uimask_ratio=uimask_ratio,\n",
    "    uimask_rand=uimask_rand,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, sample):\n",
    "    prompt_origin = 'Based on the screenshot of the page, I give an instruction and you give its corresponding location. Instruction : \"{}\"?'\n",
    "    instruction = sample['text']\n",
    "    img_path = sample['img_path']\n",
    "    prompt = prompt_origin.format(instruction)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"image\": img_path, # 高分辨率\n",
    "                },\n",
    "                # {\n",
    "                #     \"type\": \"image\",\n",
    "                #     \"image\": img_path, # 低分辨率\n",
    "                #     \"max_pixels\" : 28 * 28 * 512\n",
    "                # },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": prompt,\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    text = processor.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "    image_inputs, _ = process_vision_info(messages)\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "    with torch.inference_mode():\n",
    "        # Inference: Generation of the output\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=32, do_sample=False, temperature=None, top_p = None, top_k=None) # set greedy decoding\n",
    "        generated_ids_trimmed = [\n",
    "            out_ids[len(in_ids) :]\n",
    "            for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "        response = processor.batch_decode(\n",
    "            generated_ids_trimmed,\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=False,\n",
    "        )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mismatched_idxs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"action_type\": 4, \"click_point\": (0.24,0.35)}\\n']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference(model, naive_data[mismatched_idxs[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'img_path': '/data/data1/syc/intern/wanshan/datasets/ScreenSpot/screenspot_imgs/web_6c677961-e540-4cc5-b725-5e301019a9f9.png',\n",
       " 'text': 'bold lettering',\n",
       " 'bbox': [0.61171875, 0.5381944444444444, 0.640625, 0.5854166666666667],\n",
       " 'pred': [0.03, 0.04],\n",
       " 'matched': False,\n",
       " 'response': '{\"action_type\": 4, \"click_point\": (0.03,0.04)}\\n',\n",
       " 'type': 'icon',\n",
       " 'source': 'gitlab'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uigraph_data[mismatched_idxs[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_visualize(image_path):\n",
    "    messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": image_path,\n",
    "                # \"min_pixels\": min_pixel,\n",
    "                # \"max_pixels\": max_pixel,\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        ],\n",
    "    }\n",
    "    ]\n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    image_inputs, _ = process_vision_info(messages)\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "        vis_dir=\"./visualize_imgs\" # this folder to save visualization \n",
    "    )\n",
    "    with open(\"./visualize_imgs/demo.png\", \"rb\") as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_click_point(s):\n",
    "    \"\"\"\n",
    "    Extracts the value of 'click_point' from a string.\n",
    "\n",
    "    Parameters:\n",
    "        s (str): A string containing 'click_point' data.\n",
    "                 Expected to include a pattern like: \"click_point\": (x, y)\n",
    "\n",
    "    Returns:\n",
    "        tuple: The extracted (x, y) tuple from click_point.\n",
    "    \"\"\"\n",
    "    # Regular expression to match 'click_point\": (number, number)'\n",
    "    pattern = r'\"(?:click_point|point)\"\\s*:\\s*(\\([^)]*\\))'\n",
    "    match = re.search(pattern, s)\n",
    "    \n",
    "    if not match:\n",
    "        raise ValueError(\"The input string does not contain a valid 'click_point' pattern.\")\n",
    "    \n",
    "    click_point_str = match.group(1)\n",
    "    \n",
    "    # try:\n",
    "        # Safely evaluate to a tuple\n",
    "    click_point = ast.literal_eval(click_point_str)\n",
    "    # except Exception as e:\n",
    "    #     raise ValueError(f\"Error evaluating click_point tuple: {e}\")\n",
    "    \n",
    "    return click_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c16b9a510a4e06a417251537a5782d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75bcb34d65544642a1e8dbfe2fd261df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'', width='1000')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f5e16509b1b4d49a37aa162883f5e28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='Next Step', style=ButtonStyle()), Button(button_style='info', description='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Assumes mismatched_idxs, naive_data, uigraph_data, load_visualize, draw_click_point are defined earlier\n",
    "\n",
    "# Directory to save annotated images\n",
    "save_dir = \"./saved_preds\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Initialize sample index\n",
    "sample_idx = 0\n",
    "\n",
    "# Widgets for display\n",
    "sentence_label = widgets.HTML()\n",
    "image_widget = widgets.Image(format='png', width=1000)\n",
    "\n",
    "# Hold the latest generated image bytes for saving\n",
    "tcurrent_image_bytes = None\n",
    "\n",
    "# current image name, use later when save annotation\n",
    "current_image_name = None\n",
    "\n",
    "# Function to update display\n",
    "def update_display():\n",
    "    global sample_idx, current_image_bytes, current_image_name\n",
    "    if sample_idx < len(mismatched_idxs):\n",
    "        idx = mismatched_idxs[sample_idx]\n",
    "        naive_sample = naive_data[idx]\n",
    "        uigraph_sample = uigraph_data[idx]\n",
    "        pred = inference(model, uigraph_sample)\n",
    "\n",
    "        # Show the user instruction or text\n",
    "        instruction = naive_sample.get('text', '')\n",
    "        sentence_label.value = f\"**Naive:** {naive_sample} <br> **UIGRAPH:**{pred}\"\n",
    "\n",
    "        # Extract relative click coordinates\n",
    "        try:\n",
    "            nx, ny = naive_sample.get('pred', (0, 0))\n",
    "            # ux, uy = uigraph_sample.get('pred', (0, 0))\n",
    "            # pred = inference(model, uigraph_sample)\n",
    "            ux, uy = extract_click_point(pred[0])\n",
    "        except Exception:\n",
    "            nx = ny = ux = uy = 0\n",
    "\n",
    "        # Ground-truth bounding box\n",
    "        bbox = naive_sample.get('bbox', [0,0,0,0])\n",
    "        img_path = naive_sample.get('img_path', '')\n",
    "        current_image_name = os.path.basename(img_path)\n",
    "\n",
    "        # Optionally visualize raw GUI\n",
    "        _ = load_visualize(img_path)\n",
    "\n",
    "        # Draw both Naive (blue) and UI-Graph (red) predictions on one image\n",
    "        current_image_bytes = draw_click_point(\n",
    "            \"./visualize_imgs/demo.png\",\n",
    "            nx, ny,\n",
    "            ux, uy,\n",
    "            bbox,\n",
    "            instruction\n",
    "        )\n",
    "\n",
    "        # Update the image widget\n",
    "        image_widget.value = current_image_bytes\n",
    "\n",
    "    else:\n",
    "        # No more samples\n",
    "        sentence_label.value = \"\"\n",
    "        image_widget.value = b\"\"\n",
    "\n",
    "# Button callbacks\n",
    "def next_step(_):\n",
    "    global sample_idx\n",
    "    if sample_idx < len(mismatched_idxs) - 1:\n",
    "        sample_idx += 1\n",
    "    update_display()\n",
    "\n",
    "def save_total_incorrect(_):\n",
    "    out_path = os.path.join(save_dir, f\"totally_incorrect-{current_image_name}\")\n",
    "    with open(out_path, \"wb\") as f:\n",
    "        f.write(current_image_bytes)\n",
    "    print(f\"Totally incorrect annotated image saved to {out_path}\")\n",
    "\n",
    "def save_position_incorrect(_):\n",
    "    out_path = os.path.join(save_dir, f\"position_incorrect-{current_image_name}\")\n",
    "    with open(out_path, \"wb\") as f:\n",
    "        f.write(current_image_bytes)\n",
    "    print(f\"Position incorrect annotated image saved to {out_path}\")\n",
    "\n",
    "# Create buttons\n",
    "next_button = widgets.Button(description=\"Next Step\")\n",
    "next_button.on_click(next_step)\n",
    "\n",
    "save_btn_total_inco = widgets.Button(description=\"Save Totally Incorrect\", button_style='info')\n",
    "save_btn_total_inco.on_click(save_total_incorrect)\n",
    "\n",
    "save_btn_position_inco = widgets.Button(description=\"Save Position Incorrect\", button_style='success')\n",
    "save_btn_position_inco.on_click(save_position_incorrect)\n",
    "\n",
    "# Layout\n",
    "controls = widgets.HBox([next_button, save_btn_total_inco, save_btn_position_inco])\n",
    "\n",
    "display(sentence_label, image_widget, controls)\n",
    "# Initial display\n",
    "update_display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double Image Show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Function to draw click point on image\n",
    "# def draw_click_point(img_path, click_x, click_y, bbox, pred=False):\n",
    "#     \"\"\"\n",
    "#     img_path: str\n",
    "#     click_x, click_y : ralative coordinate (0-1)\n",
    "#     bbox, list : [x_low, y_low, x_high, y_high] (0-1)\n",
    "#     pred : model prediction | ground truth\n",
    "#         if ground truth, no output visualized image\n",
    "#     \"\"\"\n",
    "#     if os.path.exists(img_path):\n",
    "#         img = Image.open(img_path)\n",
    "#         w, h = img.size  # Get image dimensions\n",
    "        \n",
    "#         # Convert relative to absolute coordinates\n",
    "#         abs_x = int(click_x * w)\n",
    "#         abs_y = int(click_y * h)\n",
    "        \n",
    "#         # Draw the dot\n",
    "#         draw = ImageDraw.Draw(img)\n",
    "#         dot_radius = 10  # Adjust dot size if needed\n",
    "#         draw.ellipse((abs_x - dot_radius, abs_y - dot_radius, abs_x + dot_radius, abs_y + dot_radius), fill=\"red\")\n",
    "\n",
    "#         # Draw the bounding box (if exists)\n",
    "#         if bbox: # [0.278, 0.64, 0.528, 0.688]\n",
    "#             bbox_x_top_left = round(bbox[0] * w)\n",
    "#             bbox_y_top_left = round(bbox[1] * h)\n",
    "#             bbox_x_bot_right = round(bbox[2] * w)\n",
    "#             bbox_y_bot_right = round(bbox[3] * h)\n",
    "            \n",
    "#             bbox_coords = [(bbox_x_top_left, bbox_y_top_left), (bbox_x_bot_right, bbox_y_bot_right)]\n",
    "#             draw.rectangle(bbox_coords, outline=\"blue\", width=3)  # Blue bbox\n",
    "\n",
    "#         # Save the modified image temporarily\n",
    "#         if not pred:\n",
    "#             temp_img_path = \"./visualize_imgs/image.png\"\n",
    "#         else:\n",
    "#             temp_img_path = \"./visualize_imgs/pred_image.png\"\n",
    "\n",
    "#         img.save(temp_img_path)\n",
    "#         # Return the new image path\n",
    "#         return temp_img_path\n",
    "#     else:\n",
    "#         return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import shutil\n",
    "# import ipywidgets as widgets\n",
    "# from IPython.display import display\n",
    "\n",
    "# # Assumes mismatched_idxs, naive_data, uigraph_data, load_visualize, draw_click_point are defined earlier\n",
    "\n",
    "# # Directory to save predictions\n",
    "# save_dir = \"./saved_preds\"\n",
    "# os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# # Initialize sample index\n",
    "# sample_idx = 0\n",
    "\n",
    "# # Widgets for display\n",
    "# sentence_label = widgets.HTML()\n",
    "# sentece_uigraph_label = widgets.HTML()\n",
    "# image_widget = widgets.Image(format='png', width=600)\n",
    "# image_pred = widgets.Image(format='png', width=600)\n",
    "\n",
    "# # Function to update display\n",
    "# def update_display():\n",
    "#     global sample_idx, naive_image_path, uigraph_image_path\n",
    "#     if sample_idx < len(mismatched_idxs):\n",
    "#         idx = mismatched_idxs[sample_idx]\n",
    "#         naive_sample = naive_data[idx]\n",
    "#         uigraph_sample = uigraph_data[idx]\n",
    "\n",
    "#         sentence_label.value = f\"**Naive Action(s):** {naive_sample}\"\n",
    "#         sentece_uigraph_label.value = f\"**UI Graph Action(s):** {uigraph_sample}\"\n",
    "\n",
    "#         try:\n",
    "#             click_x, click_y = naive_sample.get(\"pred\", (0, 0))\n",
    "#             uigraph_click_x, uigraph_click_y = uigraph_sample.get(\"pred\", (0, 0))\n",
    "#         except Exception:\n",
    "#             click_x, click_y = 0, 0\n",
    "#             uigraph_click_x, uigraph_click_y = 0, 0\n",
    "\n",
    "#         bbox = naive_sample['bbox']\n",
    "#         img_path = naive_sample[\"img_path\"]\n",
    "\n",
    "#         # Generate annotated images\n",
    "#         _ = load_visualize(img_path)\n",
    "#         naive_image_path = draw_click_point(img_path, click_x, click_y, bbox)\n",
    "#         uigraph_image_path = draw_click_point(\"./visualize_imgs/demo.png\", uigraph_click_x, uigraph_click_y, bbox, pred=True)\n",
    "\n",
    "#         # Load images into widgets\n",
    "#         if os.path.exists(naive_image_path) and os.path.exists(uigraph_image_path):\n",
    "#             with open(naive_image_path, \"rb\") as f:\n",
    "#                 image_widget.value = f.read()\n",
    "#             with open(uigraph_image_path, \"rb\") as f:\n",
    "#                 image_pred.value = f.read()\n",
    "#         else:\n",
    "#             sentence_label.value += f\"\\n(Error: Image not found)\"\n",
    "#     else:\n",
    "#         sentence_label.value = \"\"\n",
    "#         image_widget.value = b\"\"\n",
    "#         image_pred.value = b\"\"\n",
    "\n",
    "# # Button callbacks\n",
    "# def next_step(_):\n",
    "#     global sample_idx\n",
    "#     if sample_idx < len(mismatched_idxs) - 1:\n",
    "#         sample_idx += 1\n",
    "#     update_display()\n",
    "\n",
    "# def save_naive(_):\n",
    "#     out_path = os.path.join(save_dir, f\"naive_pred_{sample_idx}.png\")\n",
    "#     shutil.copy(naive_image_path, out_path)\n",
    "#     print(f\"Naive prediction saved to {out_path}\")\n",
    "\n",
    "# def save_uigraph(_):\n",
    "#     out_path = os.path.join(save_dir, f\"uigraph_pred_{sample_idx}.png\")\n",
    "#     shutil.copy(uigraph_image_path, out_path)\n",
    "#     print(f\"UI-Graph prediction saved to {out_path}\")\n",
    "\n",
    "# # Create buttons\n",
    "# next_button = widgets.Button(description=\"Next Step\")\n",
    "# next_button.on_click(next_step)\n",
    "\n",
    "# save_naive_btn = widgets.Button(description=\"Save Naive Pred\", button_style='info')\n",
    "# save_naive_btn.on_click(save_naive)\n",
    "\n",
    "# save_uigraph_btn = widgets.Button(description=\"Save UI-Graph Pred\", button_style='success')\n",
    "# save_uigraph_btn.on_click(save_uigraph)\n",
    "\n",
    "# # Layout\n",
    "# image_box = widgets.HBox([image_widget, image_pred])\n",
    "# controls = widgets.HBox([next_button, save_naive_btn, save_uigraph_btn])\n",
    "\n",
    "# # Initial display and render\n",
    "# update_display()\n",
    "# display(sentence_label, sentece_uigraph_label, image_box, controls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen2vl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
