{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb;\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import torch\n",
    "import PIL.Image as Image\n",
    "\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qwen_vl_utils import process_vision_info\n",
    "from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor\n",
    "\n",
    "# 0. Common setups\n",
    "min_pixels = 256*28*28\n",
    "max_pixels = 1344*28*28\n",
    "\n",
    "model_path = \"/data/data1/syc/intern/wanshan/models/Qwen2-VL-2B-Instruct\"\n",
    "processor = Qwen2VLProcessor.from_pretrained(\n",
    "    model_path, \n",
    "    min_pixels=min_pixels, max_pixels=max_pixels,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2VLConfig\n",
    "config = Qwen2VLConfig.from_pretrained(model_path)\n",
    "print(config._attn_implementation) # only eager attention implementation can output attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_qwen_layer = 28 # LLM Decoder layers\n",
    "device = 'cuda:3'\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=device,\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_url = 'examples/test.jpg'\n",
    "vis_dir = 'examples'\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Please describe this image.\"},\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": img_url,\n",
    "                \"min_pixels\": min_pixels,\n",
    "                \"max_pixels\": max_pixels,\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True,\n",
    ")\n",
    "print(text)\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "inputs = inputs.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in inputs.keys():\n",
    "    print(key, inputs[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_inputs['patch_pos'] = torch.zeros_like(text_inputs['input_ids']) -1\n",
    "vision_idx = {\n",
    "    'start': 0,\n",
    "    'end': 0\n",
    "}\n",
    "for i in range(len(inputs['input_ids'][0])):\n",
    "    # assume here is 1 x L\n",
    "    if inputs['input_ids'][0, i] == 151652:   # <|vision_start|> in Qwen2VL vocabulary\n",
    "        vision_idx['start'] = i\n",
    "        print(f'<vision_state> at {i}')\n",
    "    if inputs['input_ids'][0, i] == 151653:   # <|vision_end|> in Qwen2VL vocabulary\n",
    "        print(f'<vision_end> at {i}')\n",
    "        vision_end = i\n",
    "        vision_idx['end'] = i\n",
    "\n",
    "vision_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = model.generate(\n",
    "                    **inputs,\n",
    "                    do_sample=False,\n",
    "                    max_new_tokens=128,\n",
    "                    use_cache=True,\n",
    "                    # stopping_criteria=[stopping_criteria],\n",
    "                    output_attentions=True,\n",
    "                    output_scores=True,\n",
    "                    return_dict_in_generate=True,\n",
    "                    )\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids['sequences'])\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, \n",
    ")[0]\n",
    "print(output_text)\n",
    "for key in generated_ids.keys():\n",
    "    print(key, type(generated_ids[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_seqs = generated_ids['sequences']\n",
    "output_seqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_outputs = generated_ids['attentions']\n",
    "print(len(attention_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = generated_ids['scores']\n",
    "print(len(scores[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in generated_ids.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as Colormap\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "# refer to fastv paper\n",
    "def visualize_attention(multihead_attention,output_path=\"atten_map_1.png\",title=\"Layer 5\", vision_idx=None):\n",
    "    print(multihead_attention.shape)\n",
    "    # multihead_attention (1, num_heads, n_tokens, n_tokens)\n",
    "    # First, we average the attention scores over the multiple heads\n",
    "    pool_stride = 20\n",
    "    averaged_attention = torch.mean(multihead_attention, axis=1)[0].float()# Shape: (n_tokens, n_tokens)\n",
    "    \n",
    "    # pooling the attention scores  with stride = pool_stride\n",
    "    # avg_pool2d requires 4d tensor (batch_size, channels, height, width)\n",
    "    averaged_attention = torch.nn.functional.avg_pool2d(averaged_attention.unsqueeze(0).unsqueeze(0), pool_stride, stride=pool_stride).squeeze(0).squeeze(0)\n",
    "    \n",
    "    cmap = plt.get_cmap(\"viridis\")\n",
    "    plt.figure(figsize=(5, 5),dpi=400)\n",
    "\n",
    "    # Log normalization\n",
    "    log_norm = LogNorm(vmin=0.0007, vmax=0.1)\n",
    "\n",
    "    # set the x and y ticks to 20x of the original\n",
    "\n",
    "    ax = sns.heatmap(averaged_attention,\n",
    "                cmap=cmap,  # custom color map\n",
    "                norm=log_norm,  # \n",
    "                cbar_kws={'label': 'Attention score'},\n",
    "                )\n",
    "    \n",
    "    # replace the x and y ticks with string\n",
    "\n",
    "    x_ticks = [str(i*pool_stride) for i in range(0,averaged_attention.shape[0]+1)]\n",
    "    y_ticks = [str(i*pool_stride) for i in range(0,averaged_attention.shape[0])]\n",
    "    ax.set_xticks([i for i in range(0,averaged_attention.shape[0]+1)])\n",
    "    ax.set_yticks([i for i in range(0,averaged_attention.shape[0])])\n",
    "    ax.set_xticklabels(x_ticks)\n",
    "    ax.set_yticklabels(y_ticks)\n",
    "\n",
    "    # change the x tinks font size\n",
    "    plt.xticks(fontsize=3)\n",
    "    plt.yticks(fontsize=3)\n",
    "    \n",
    "    # make y label vertical\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.xticks(rotation=90)    \n",
    "\n",
    "    # remove the x and y ticks\n",
    "    plt.xticks([])\n",
    "    plt.yticks([]) \n",
    "\n",
    "    # draw horizontal lines for <vision_start> to <visual_end>\n",
    "    # ax.text(x=vision_idx['start'] // pool_stride + 1, y=vision_idx['start'] // pool_stride, s='<vision_start>', color='b', fontsize=4)\n",
    "    # ax.text(x=vision_idx['end'] // pool_stride -1 , y=vision_idx['start'] // pool_stride, s='<vision_end>', color='b', fontsize=4)\n",
    "    ax.axvline(x=vision_idx['start'] // pool_stride + 1, color='r', linestyle='--', linewidth=0.3)\n",
    "    ax.axhline(y=vision_idx['start'] // pool_stride + 1, color='r', linestyle='--', linewidth=0.3)\n",
    "    ax.axvline(x=vision_idx['end'] // pool_stride, color='r', linestyle='--', linewidth=0.3)\n",
    "    ax.axhline(y=vision_idx['end'] // pool_stride, color='r', linestyle='--', linewidth=0.3)\n",
    "    \n",
    "    plt.title(title)\n",
    "    # tight layout\n",
    "    plt.savefig(output_path, bbox_inches='tight')\n",
    "    # plt.close() # close the plot, without displaying it in the notebook\n",
    "    plt.show()\n",
    "\n",
    "    top_five_attentions = []\n",
    "    for row in averaged_attention:\n",
    "        # Use torch.topk to get the top 5 values and their indices\n",
    "        top_values, top_indices = torch.topk(row, 10)\n",
    "        # Convert to lists and append to the overall list\n",
    "        top_five_line = list(zip(top_indices.tolist(), top_values.tolist()))\n",
    "        top_five_attentions.append(top_five_line)\n",
    "        \n",
    "    return top_five_attentions,averaged_attention    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, attention in enumerate(attention_outputs[0]):\n",
    "    # print(i, attention)\n",
    "    # print(top5_attention)\n",
    "    top5_attention,average_attentions = visualize_attention(attention.cpu(), output_path=\"./attn_maps/atten_map_\"+str(i+1)+\".png\",title=\"Layer \"+str(i+1), vision_idx=vision_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "show-ui",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
