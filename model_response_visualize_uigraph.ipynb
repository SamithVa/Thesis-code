{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Image\n",
    "import os\n",
    "from Qwen2VL_ui_graph.model.processing_qwen2_vl import Qwen2VLProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import ast  # To safely evaluate JSON-like strings\n",
    "from PIL import Image, ImageDraw\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to draw click point on image\n",
    "def draw_click_point(img_path, click_x, click_y, bbox, pred=False):\n",
    "    if os.path.exists(img_path):\n",
    "        img = Image.open(img_path)\n",
    "        w, h = img.size  # Get image dimensions\n",
    "        \n",
    "        # Convert relative to absolute coordinates\n",
    "        abs_x = int(click_x * w)\n",
    "        abs_y = int(click_y * h)\n",
    "        \n",
    "        # Draw the dot\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        dot_radius = 10  # Adjust dot size if needed\n",
    "        draw.ellipse((abs_x - dot_radius, abs_y - dot_radius, abs_x + dot_radius, abs_y + dot_radius), fill=\"red\")\n",
    "\n",
    "        # Draw the bounding box (if exists)\n",
    "        if bbox: # [0.278, 0.64, 0.528, 0.688]\n",
    "            bbox_x_top_left = round(bbox[0] * w)\n",
    "            bbox_y_top_left = round(bbox[1] * h)\n",
    "            bbox_x_bot_right = round(bbox[2] * w)\n",
    "            bbox_y_bot_right = round(bbox[3] * h)\n",
    "            \n",
    "            bbox_coords = [(bbox_x_top_left, bbox_y_top_left), (bbox_x_bot_right, bbox_y_bot_right)]\n",
    "            draw.rectangle(bbox_coords, outline=\"blue\", width=3)  # Blue bbox\n",
    "\n",
    "        # Save the modified image temporarily\n",
    "        if not pred:\n",
    "            temp_img_path = \"./visualize_imgs/image.png\"\n",
    "        else:\n",
    "            temp_img_path = \"./visualize_imgs/pred_image.png\"\n",
    "\n",
    "        img.save(temp_img_path)\n",
    "        # Return the new image path\n",
    "        return temp_img_path\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON file\n",
    "naive_json_path = \"/home/syc/intern/wanshan/Thesis_result/Mind2Web/Text-History/Qwen2-VL-7B_naive/qwen2vl_resampler_7b_keep_1_date_0406_website.json\"  # Change this to your JSON file path\n",
    "# json_file = \"/home/syc/intern/wanshan/Qwen2-VL/agent_tasks/custom_training_script/qwen2vl_train_train.json\"\n",
    "with open(naive_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "# Filter instances where Ele_match is False\n",
    "naive_data = [\n",
    "    [step for step in episode] \n",
    "    for episode in data\n",
    "]\n",
    "\n",
    "# Initialize index\n",
    "episode_index = 0\n",
    "step_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "uigraph_json_path = \"/home/syc/intern/wanshan/Qwen2-VL/agent_tasks/qwen2vl-7b_uigraph_infer_0.2_website.json\"\n",
    "with open(uigraph_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "# Filter instances where Ele_match is False\n",
    "uigraph_data = [\n",
    "    [step for step in episode] \n",
    "    for episode in data\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(naive_data[0]), len(uigraph_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.278, 0.64, 0.528, 0.688]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ground_truth_path = \"/home/syc/intern/wanshan/Qwen2-VL/data/subset_100_samples.json\"\n",
    "# with open(ground_truth_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#     ground_truth_data = json.load(f)\n",
    "\n",
    "naive_data[0][0]['bbox_ref']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "navigation_sequence  = []\n",
    "for episode_idx, episode in enumerate(naive_data):\n",
    "    for step_idx, step in enumerate(episode):\n",
    "        if step['Ele_match'] == True and uigraph_data[episode_idx][step_idx]['Ele_match'] == False:\n",
    "            navigation_sequence.append((episode_idx, step_idx))\n",
    "sequence_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(navigation_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/data/data1/syc/intern/wanshan/models/Qwen2-VL-2B-Instruct\"\n",
    "# model_path = \"/data/data1/syc/intern/wanshan/models/showlab/ShowUI-2B_edited\"\n",
    "\n",
    "min_pixel = 1344*28*28\n",
    "max_pixel = 1680*28*28\n",
    "# 1. Screenshot -> Graph\n",
    "uigraph_train = True        # Enable ui graph during training\n",
    "uigraph_test = True         # Enable ui graph during inference\n",
    "uigraph_diff = 1            # Pixel difference used for constructing ui graph\n",
    "uigraph_rand = False        # Enable random graph construction \n",
    "# 2. Graph -> Mask \n",
    "uimask_pre = True           # Prebuild patch selection mask in the preprocessor (not in model layers) for efficiency\n",
    "uimask_ratio = 0.2         # Specify the percentage of patch tokens to skip per component\n",
    "uimask_rand = False         # Enable random token selection instead of uniform selection\n",
    "\n",
    "\n",
    "processor = Qwen2VLProcessor.from_pretrained(\n",
    "        model_path,\n",
    "        min_pixels= min_pixel,\n",
    "        max_pixels = max_pixel,\n",
    "        uigraph_train=uigraph_train, uigraph_test=uigraph_test, uigraph_diff=uigraph_diff, uigraph_rand=uigraph_rand,\n",
    "        uimask_pre=True, uimask_ratio=uimask_ratio, uimask_rand=uimask_rand,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_visualize(image_path):\n",
    "    messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": image_path,\n",
    "                \"min_pixels\": min_pixel,\n",
    "                \"max_pixels\": max_pixel,\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        ],\n",
    "    }\n",
    "    ]\n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    image_inputs, _ = process_vision_info(messages)\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "        vis_dir=\"./visualize_imgs\" # this folder to save visualization \n",
    "    )\n",
    "    with open(\"./visualize_imgs/demo.png\", \"rb\") as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c34d59e678746da880787da9d6b1644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='**Naive Action(s):** {\\'annot_id\\': \\'013781df-4391-4533-bcb1-15f6819064f6\\', \\'img_path\\': \\'/dat…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3d112f5a63648c1aeb3debc76aa068d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='**UI Graph Action(s):** {\\'annot_id\\': \\'013781df-4391-4533-bcb1-15f6819064f6\\', \\'img_path\\': \\'/…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aea6acac3f5c465d96657538b3106744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x05\\x00\\x00\\x00\\x02\\xd0\\x08\\x02\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b45121d390514ace924f1963c52aa40f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Next Step', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Function to update display\n",
    "def update_display():\n",
    "    global sequence_index\n",
    "    \n",
    "    if sequence_index < len(navigation_sequence):\n",
    "        episode_index, step_index = navigation_sequence[sequence_index]\n",
    "        episode = naive_data[episode_index]\n",
    "        episode_uigraph = uigraph_data[episode_index]\n",
    "        # episode_ref = ground_truth_bbox[episode_index]\n",
    "\n",
    "        if step_index < len(episode):\n",
    "            step = episode[step_index]\n",
    "            step_uigraph =  episode_uigraph[step_index]\n",
    "            # step_ref = episode_ref[step_index]\n",
    "            \n",
    "            # Extract sentence (convert list of JSON strings into readable text)\n",
    "            # sentence_text = \"\\n\".join(step[\"sentence\"])\n",
    "            \n",
    "            # Update instruction and sentence\n",
    "            # instruction_label.value = f\"**Instruction:** {step['instruction']}\"\n",
    "            sentence_label.value = f\"**Naive Action(s):** {step}\"\n",
    "            sentece_uigraph_label.value = f\"**UI Graph Action(s):** {step_uigraph}\"\n",
    "            # Extract click coordinates\n",
    "            try:\n",
    "                action_data = ast.literal_eval(step[\"sentence\"][0])  # Convert string to dict\n",
    "                click_x, click_y = action_data.get(\"click_point\", (0, 0))\n",
    "\n",
    "                action_data_uigraph = ast.literal_eval(step_uigraph[\"sentence\"][0])  # Convert string to dict\n",
    "                uigraph_click_x, uigraph_click_y = action_data_uigraph.get(\"click_point\", (0, 0))\n",
    "                # uigraph_click_y += 0.02\n",
    "            except Exception as e:\n",
    "                click_x, click_y = 0, 0  # Default to top-left corner on error\n",
    "\n",
    "            # get bbox from groundtruth\n",
    "            bbox = step['bbox_ref']\n",
    "            \n",
    "            # Load and update image\n",
    "            img_path = step[\"img_path\"]\n",
    "            img_pred_path = step_uigraph[\"img_path\"]\n",
    "            \n",
    "            _ = load_visualize(img_path)\n",
    "            modified_img_path = draw_click_point(img_path, click_x, click_y, bbox)\n",
    "            modified_img_pred_path = draw_click_point(\"./visualize_imgs/demo.png\", uigraph_click_x, uigraph_click_y, bbox, pred=True)\n",
    "            if os.path.exists(img_path):\n",
    "                with open(modified_img_path, \"rb\") as f:\n",
    "                    image_widget.value = f.read()\n",
    "                with open(modified_img_pred_path, \"rb\") as f:\n",
    "                    image_pred.value = f.read()\n",
    "                # image_patch.value = load_visualize(img_path)\n",
    "            else:\n",
    "                sentence_label.value += f\"\\n(Error: Image not found at {img_path})\"\n",
    "        else:\n",
    "            sentence_label.value = \"No more steps in this episode.\"\n",
    "            image_widget.value = b\"\"\n",
    "    else:\n",
    "        # instruction_label.value = \"No more episodes.\"\n",
    "        sentence_label.value = \"\"\n",
    "        image_widget.value = b\"\"\n",
    "\n",
    "# Next button function\n",
    "def next_step(_):\n",
    "    global sequence_index\n",
    "    \n",
    "    # Move to the next item in the navigation sequence\n",
    "    if sequence_index < len(navigation_sequence) - 1:\n",
    "        sequence_index += 1\n",
    "        update_display()\n",
    "\n",
    "\n",
    "    update_display()\n",
    "\n",
    "# Widgets\n",
    "# instruction_label = widgets.HTML()\n",
    "sentence_label = widgets.HTML()\n",
    "sentece_uigraph_label = widgets.HTML()\n",
    "image_widget = widgets.Image(format='png', width=600)  # Set Image Size\n",
    "image_pred = widgets.Image(format='png', width=600)\n",
    "# image_patch = widgets.Image(format='png', width=600)  # Set Image Size\n",
    "\n",
    "# Layout to show images side by side\n",
    "image_box = widgets.HBox([image_widget, image_pred])  # Side-by-side\n",
    "\n",
    "\n",
    "next_button = widgets.Button(description=\"Next Step\")\n",
    "next_button.on_click(next_step)\n",
    "\n",
    "# Initial display\n",
    "update_display()\n",
    "\n",
    "# Layout\n",
    "# display(sentence_label, sentece_uigraph_label,  image_box, image_patch, next_button)\n",
    "display(sentence_label, sentece_uigraph_label,  image_box, next_button)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen2vl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
