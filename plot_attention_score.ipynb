{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb;\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import torch\n",
    "import PIL.Image as Image\n",
    "\n",
    "from Qwen2VL_uigraph.model import Qwen2VLForConditionalGeneration, Qwen2VLProcessor\n",
    "# torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "# torch.backends.cuda.enable_flash_sdp(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resize image using PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# img = Image.open('chrome.png', 'r')\n",
    "# w, h = img.size \n",
    "# ratio = w / h\n",
    "# new_h = 720\n",
    "# new_w = round(720 * ratio)\n",
    "# print(new_w, new_h, new_w * new_h // 28 // 28)\n",
    "# img_resize = img.resize((new_w, new_h))\n",
    "# img_resize.save('chrome_resized.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qwen_vl_utils import process_vision_info\n",
    "# from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor\n",
    "\n",
    "# from model.showui.modeling_showui import ShowUIForConditionalGeneration\n",
    "# from model.showui.processing_showui import ShowUIProcessor\n",
    "\n",
    "### ShowUI Preprocessor\n",
    "# 0. Common setups\n",
    "min_pixels = 256*28*28\n",
    "max_pixels = 1344*28*28\n",
    "# 1. Screenshot -> Graph\n",
    "uigraph_train = True        # Enable ui graph during training\n",
    "uigraph_test = True         # Enable ui graph during inference\n",
    "uigraph_diff = 1            # Pixel difference used for constructing ui graph\n",
    "uigraph_rand = False        # Enable random graph construction \n",
    "# 2. Graph -> Mask \n",
    "uimask_pre = True           # Prebuild patch selection mask in the preprocessor (not in model layers) for efficiency\n",
    "uimask_ratio = 0.5          # Specify the percentage of patch tokens to skip per component\n",
    "uimask_rand = False         # Enable random token selection instead of uniform selection\n",
    "\n",
    "### ShowUI Model\n",
    "lm_skip_ratio = uimask_ratio # valid if not uimask_pre\n",
    "lm_skip_layer = \"[1,28,1]\"   # [1,28,1] means we apply UI guide token selection from 1-th to 28-th layer (28 is the last layer of Qwen2-VL)\n",
    "model_path = \"/data/data1/syc/intern/wanshan/models/Qwen2-VL-2B-Instruct\"\n",
    "\n",
    "processor = Qwen2VLProcessor.from_pretrained(\n",
    "    model_path, \n",
    "    min_pixels=min_pixels, max_pixels=max_pixels,\n",
    "    uigraph_train=uigraph_train, uigraph_test=uigraph_test, uigraph_diff=uigraph_diff, uigraph_rand=uigraph_rand,\n",
    "    uimask_pre=True, uimask_ratio=uimask_ratio, uimask_rand=uimask_rand,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eager\n"
     ]
    }
   ],
   "source": [
    "from transformers import Qwen2VLConfig\n",
    "config = Qwen2VLConfig.from_pretrained(model_path)\n",
    "print(config._attn_implementation) # only eager attention implementation can output attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,28,1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a76031b9bfa4a24b1a352cb1a0855b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm_qwen_layer = 28 # LLM Decoder layers\n",
    "def parse_layer_type(str_ranges, L=lm_qwen_layer, default=0):\n",
    "    # 0 is without layer token selection, 1 is with layer token selection. Below we provide examples:\n",
    "    # [1,28,1] means that all LM layers use token selection; [1,28,0] means that do not.\n",
    "    # Interleaved layer-wise '[2,2,1],[4,4,1],[6,6,1],[8,8,1],[10,10,1],[12,12,1],[14,14,1],[16,16,1],[18,18,1],[20,20,1],[22,22,1],[24,24,1],[26,26,1]'\n",
    "    result = [default] * L\n",
    "    matches = re.findall(r'\\[\\s*(\\d+)\\s*,\\s*(\\d+)\\s*,\\s*(\\d+)\\s*\\]', str_ranges)\n",
    "    for start, end, value in matches:\n",
    "        start, end, value = int(start) - 1, int(end) - 1, int(value)\n",
    "        if end >= L:\n",
    "            end = L - 1\n",
    "        result[start:end + 1] = [value] * (end - start + 1)\n",
    "    return result\n",
    "\n",
    "lm_skip_layer_arr = parse_layer_type(lm_skip_layer, 28)\n",
    "print(lm_skip_layer)\n",
    "device = 'cuda'\n",
    "\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=device,\n",
    "    lm_skip_ratio=lm_skip_ratio, \n",
    "    lm_skip_layer=lm_skip_layer_arr,\n",
    "    attn_implementation=\"eager\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Please describe this image in detail. Explain what is depicted, including all visible components, structures, or elements, and how they interact with each other. Clarify the purpose of this object or system and the context in which it is typically used. Additionally, explain why this is important — what problems it helps solve, what needs it fulfills, or what advantages it provides. If applicable, describe the technology behind it, its functionality, and its role in broader applications. Your response should be approximately 100 words, providing a comprehensive and informative explanation for better understanding.<|vision_start|><|image_pad|><|vision_end|><|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "img_url = 'chrome_resized.png'\n",
    "vis_dir = 'visualize_imgs'\n",
    "\n",
    "max_pixels = 1024 * 28 * 28\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Please describe this image in detail. Explain what is depicted, including all visible components, structures, or elements, \"\n",
    "            \"and how they interact with each other. Clarify the purpose of this object or system and the context in which it is typically used. \"\n",
    "            \"Additionally, explain why this is important — what problems it helps solve, what needs it fulfills, or what advantages it provides. \"\n",
    "            \"If applicable, describe the technology behind it, its functionality, and its role in broader applications. \"\n",
    "            \"Your response should be approximately 100 words, providing a comprehensive and informative explanation for better understanding.\"},\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": img_url,\n",
    "                \"min_pixels\": min_pixels,\n",
    "                \"max_pixels\": max_pixels,\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True,\n",
    ")\n",
    "print(text)\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "inputs = inputs.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([1, 1136])\n",
      "attention_mask torch.Size([1, 1136])\n",
      "patch_pos torch.Size([1, 1136])\n",
      "select_mask torch.Size([1, 1136])\n",
      "pixel_values torch.Size([4000, 1176])\n",
      "image_grid_thw torch.Size([1, 3])\n",
      "patch_assign torch.Size([1000])\n",
      "patch_assign_sep torch.Size([1000])\n",
      "patch_assign_len torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for key in inputs.keys():\n",
    "    print(key, inputs[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(inputs[\"select_mask\"].sum(), inputs['select_mask'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151653"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.vision_end_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<vision_state> at 129\n",
      "<vision_end> at 1130\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'start': 130, 'end': 1130}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text_inputs['patch_pos'] = torch.zeros_like(text_inputs['input_ids']) -1\n",
    "vision_idx = {\n",
    "    'start': 0,\n",
    "    'end': 0\n",
    "}\n",
    "\n",
    "for i in range(len(inputs['input_ids'][0])):\n",
    "    # assume here is 1 x L\n",
    "    if inputs['input_ids'][0, i] == config.vision_start_token_id:  \n",
    "        vision_idx['start'] = i+1\n",
    "        print(f'<vision_state> at {i}') # vision_start, image_tokens ... , vision_end\n",
    "    if inputs['input_ids'][0, i] == config.vision_end_token_id:  \n",
    "        print(f'<vision_end> at {i}')\n",
    "        vision_end = i\n",
    "        vision_idx['end'] = i\n",
    "\n",
    "vision_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/syc/anaconda3/envs/qwen2vl/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.01` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/syc/anaconda3/envs/qwen2vl/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.001` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/syc/anaconda3/envs/qwen2vl/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:651: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Qwen2VLDecoderLayer' object has no attribute 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;66;43;03m# stopping_criteria=[stopping_criteria],\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m                    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                    \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m generated_ids_trimmed \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     12\u001b[0m     out_ids[\u001b[38;5;28mlen\u001b[39m(in_ids) :] \u001b[38;5;28;01mfor\u001b[39;00m in_ids, out_ids \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(inputs\u001b[38;5;241m.\u001b[39minput_ids, generated_ids[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msequences\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     13\u001b[0m ]\n\u001b[1;32m     14\u001b[0m output_text \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mbatch_decode(\n\u001b[1;32m     15\u001b[0m     generated_ids_trimmed, \n\u001b[1;32m     16\u001b[0m )[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/qwen2vl/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/qwen2vl/lib/python3.10/site-packages/transformers/generation/utils.py:2223\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2215\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2216\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2217\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2218\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2219\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2220\u001b[0m     )\n\u001b[1;32m   2222\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2223\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2224\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2228\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2230\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2231\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2233\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2234\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2236\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2237\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2242\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2243\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/qwen2vl/lib/python3.10/site-packages/transformers/generation/utils.py:3211\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3208\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[0;32m-> 3211\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3212\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/qwen2vl/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/qwen2vl/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/intern/wanshan/llm/Qwen2VL_uigraph/model/modeling_qwen2_vl.py:2382\u001b[0m, in \u001b[0;36mQwen2VLForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, rope_deltas, cache_position, patch_assign, patch_assign_len, patch_assign_sep, patch_pos, select_mask)\u001b[0m\n\u001b[1;32m   2379\u001b[0m         position_ids \u001b[38;5;241m=\u001b[39m position_ids\u001b[38;5;241m.\u001b[39madd(delta)\n\u001b[1;32m   2380\u001b[0m         position_ids \u001b[38;5;241m=\u001b[39m position_ids\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 2382\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2383\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2387\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2388\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2389\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2390\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2393\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatch_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatch_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2394\u001b[0m \u001b[43m    \u001b[49m\u001b[43mselect_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mselect_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2395\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2397\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   2398\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/anaconda3/envs/qwen2vl/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/qwen2vl/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/intern/wanshan/llm/Qwen2VL_uigraph/model/modeling_qwen2_vl.py:1699\u001b[0m, in \u001b[0;36mQwen2VLModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, patch_pos, select_mask)\u001b[0m\n\u001b[1;32m   1685\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1686\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1687\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1696\u001b[0m         select_mask,\n\u001b[1;32m   1697\u001b[0m     )\n\u001b[1;32m   1698\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1699\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1702\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1703\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1704\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1705\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpatch_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatch_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m        \u001b[49m\u001b[43mselect_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mselect_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1712\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1714\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/envs/qwen2vl/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/qwen2vl/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/intern/wanshan/llm/Qwen2VL_uigraph/model/modeling_qwen2_vl.py:1157\u001b[0m, in \u001b[0;36mQwen2VLDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, patch_pos, select_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnavie_forward(\n\u001b[1;32m   1146\u001b[0m         hidden_states,\n\u001b[1;32m   1147\u001b[0m         attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1154\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1155\u001b[0m     )\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_skip \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m-> 1157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mui_guide_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpatch_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mselect_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1171\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "File \u001b[0;32m~/intern/wanshan/llm/Qwen2VL_uigraph/model/modeling_qwen2_vl.py:1317\u001b[0m, in \u001b[0;36mQwen2VLDecoderLayer.ui_guide_forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, patch_pos, select_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1313\u001b[0m adjusted_position_embeddings \u001b[38;5;241m=\u001b[39m (adjusted_cos, adjusted_sin)\n\u001b[1;32m   1315\u001b[0m processed_hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mclone()\n\u001b[0;32m-> 1317\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_causal_mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1318\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mselected_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madjusted_cache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m block_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnavie_forward(\n\u001b[1;32m   1326\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mselected_hidden_states,\n\u001b[1;32m   1327\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;66;03m# FIXME\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1335\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1336\u001b[0m )\n\u001b[1;32m   1338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/intern/wanshan/llm/Qwen2VL_uigraph/model/modeling_qwen2_vl.py:1373\u001b[0m, in \u001b[0;36mQwen2VLDecoderLayer._update_causal_mask\u001b[0;34m(self, attention_mask, input_tensor, cache_position, past_key_values, output_attentions)\u001b[0m\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_update_causal_mask\u001b[39m(\n\u001b[1;32m   1366\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1367\u001b[0m     attention_mask: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1371\u001b[0m     output_attentions: \u001b[38;5;28mbool\u001b[39m,\n\u001b[1;32m   1372\u001b[0m ):\n\u001b[0;32m-> 1373\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241m.\u001b[39m_attn_implementation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflash_attention_2\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1374\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01min\u001b[39;00m attention_mask:\n\u001b[1;32m   1375\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m attention_mask\n",
      "File \u001b[0;32m~/anaconda3/envs/qwen2vl/lib/python3.10/site-packages/torch/nn/modules/module.py:1931\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1931\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1932\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1933\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Qwen2VLDecoderLayer' object has no attribute 'config'"
     ]
    }
   ],
   "source": [
    "generated_ids = model.generate(\n",
    "                    **inputs,\n",
    "                    do_sample=False,\n",
    "                    max_new_tokens=128,\n",
    "                    use_cache=True,\n",
    "                    # stopping_criteria=[stopping_criteria],\n",
    "                    output_attentions=True,\n",
    "                    output_scores=True,\n",
    "                    return_dict_in_generate=True,\n",
    "                    )\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids['sequences'])\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, \n",
    ")[0]\n",
    "print(output_text)\n",
    "for key in generated_ids.keys():\n",
    "    print(key, type(generated_ids[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1202])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1330])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_seqs = generated_ids['sequences']\n",
    "output_seqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "attention_outputs = generated_ids['attentions']\n",
    "print(len(attention_outputs)) # [seq_len, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(attention_outputs[0]) # 28 decoder layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 1202, 1202])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_outputs[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 1202, 1202])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_outputs[0][27].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 1, 1203])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_outputs[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "scores = generated_ids['scores']\n",
    "print(len(scores[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 152064])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# # Suppose:\n",
    "# # C = number of input tokens (e.g., 516)\n",
    "# # G = number of generated tokens you want to combine (64)\n",
    "# C = 516\n",
    "# G = 64\n",
    "# final_seq_len = C + G\n",
    "\n",
    "# # full_attention_outputs will be a list with one tensor per layer (28 in total)\n",
    "# full_attention_outputs = []  # one per decoder layer\n",
    "\n",
    "# # Loop over each layer (assuming 28 layers)\n",
    "# for layer in range(1):\n",
    "#     # Get the cached attention for the input tokens.\n",
    "#     # This tensor has shape [1, 12, C, C].\n",
    "#     cached_attn = attention_outputs[0][layer]   # shape: [1, 12, C, C]\n",
    "#     print(cached_attn.shape)\n",
    "    \n",
    "#     # This will store the new token rows (one per generation step).\n",
    "#     new_tokens_attn = []\n",
    "    \n",
    "#     # For generation steps 1 to G (i.e., the first 64 generated tokens)\n",
    "#     for gen_idx in range(1, G + 1):\n",
    "#         # Get the new token's attention for the layer.\n",
    "#         # It should have shape [1, 12, 1, (C + gen_idx)].\n",
    "#         new_attn = attention_outputs[gen_idx][layer]\n",
    "#         print(new_attn.shape)\n",
    "#         cached_attn = torch.cat((cached_attn,new_tokens_attn), dim=2)\n",
    "#         print(cached_attn.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences\n",
      "scores\n",
      "attentions\n",
      "past_key_values\n"
     ]
    }
   ],
   "source": [
    "for key in generated_ids.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': 130, 'end': 1196}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vision_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1330])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([151644,   8948,    198,  ...,    279,   1849,   3619], device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib.colors import LogNorm, ListedColormap\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "def visualize_attention(multihead_attention, \n",
    "                        output_path=\"atten_map_1.png\",\n",
    "                        title=\"Layer 5\", \n",
    "                        vision_idx=None, \n",
    "                        show_image=False,\n",
    "                        pool_stride=20, \n",
    "                        dpi=200):\n",
    "    \"\"\"\n",
    "    Visualize a single attention map without tick labels on the main heatmap.\n",
    "    Adds thin colored strips along the LEFT and BOTTOM edges to indicate token regions:\n",
    "      - Blue: Text tokens (System+Instruction) outside vision token range\n",
    "      - Red: Vision tokens (vision_idx['start'] to vision_idx['end'])\n",
    "    Additionally, a small corner patch is placed at the bottom-left corner \n",
    "    so there is no extra unfilled square.\n",
    "\n",
    "    Parameters:\n",
    "      multihead_attention (torch.Tensor): shape (1, num_heads, n_tokens, n_tokens).\n",
    "      output_path (str): File path to save the figure.\n",
    "      title (str): Plot title.\n",
    "      vision_idx (dict or None): Dictionary with 'start' and 'end' for vision tokens.\n",
    "      show_image (bool): If True, display the plot.\n",
    "      pool_stride (int): Stride used for average pooling.\n",
    "      dpi (int): Dots per inch for the figure.\n",
    "    \n",
    "    Returns:\n",
    "      top_ten_attentions (list): A list with top 10 (index, score) pairs for each row.\n",
    "      averaged_attention (torch.Tensor): The 2D pooled attention map.\n",
    "    \"\"\"\n",
    "    print(\"multihead_attention shape:\", multihead_attention.shape)\n",
    "    \n",
    "    # 1) Average attention scores over heads\n",
    "    averaged_attention = torch.mean(multihead_attention, axis=1)[0].float()\n",
    "\n",
    "    # 2) Downsample via average pooling\n",
    "    averaged_attention = torch.nn.functional.avg_pool2d(\n",
    "        averaged_attention.unsqueeze(0).unsqueeze(0), \n",
    "        kernel_size=pool_stride, \n",
    "        stride=pool_stride\n",
    "    ).squeeze(0).squeeze(0)\n",
    "\n",
    "    # Number of pooled tokens (square shape)\n",
    "    n_cells = averaged_attention.shape[0]\n",
    "\n",
    "    # 3) Create figure & main heatmap axis (no tick labels)\n",
    "    fig, ax = plt.subplots(figsize=(10, 10), dpi=dpi)\n",
    "    \n",
    "    cmap = plt.get_cmap(\"inferno\")\n",
    "    log_norm = LogNorm(vmin=0.0007, vmax=0.1)\n",
    "    \n",
    "    hm = sns.heatmap(\n",
    "        averaged_attention,\n",
    "        cmap=cmap,\n",
    "        norm=log_norm,\n",
    "        cbar_kws={'label': 'Attention score'},\n",
    "        ax=ax,\n",
    "        xticklabels=False,\n",
    "        yticklabels=False\n",
    "    )\n",
    "    \n",
    "    if vision_idx is not None:\n",
    "        # Convert original token indices → pooled indices\n",
    "        pooled_v_start = vision_idx['start'] // pool_stride\n",
    "        pooled_v_end   = vision_idx['end']   // pool_stride\n",
    "        \n",
    "        # 0 => text token, 1 => vision token\n",
    "        token_types = np.zeros(n_cells, dtype=int)\n",
    "        token_types[pooled_v_start:pooled_v_end] = 1\n",
    "        \n",
    "        # For the strips\n",
    "        left_strip   = token_types.reshape(n_cells, 1)  # vertical\n",
    "        bottom_strip = token_types.reshape(1, n_cells)  # horizontal\n",
    "\n",
    "        # Colormap for the strips: 0=blue, 1=red\n",
    "        token_cmap = ListedColormap(['lightblue', 'lightsalmon'])\n",
    "        \n",
    "        # 4) Position these strips flush with the heatmap\n",
    "        pos = ax.get_position()\n",
    "\n",
    "        # Thinner bars\n",
    "        left_bar_width    = 0.02\n",
    "        bottom_bar_height = 0.02\n",
    "\n",
    "        # Left strip\n",
    "        left_ax = fig.add_axes([\n",
    "            pos.x0 - left_bar_width - 0.01,\n",
    "            pos.y0,\n",
    "            left_bar_width ,\n",
    "            pos.height\n",
    "        ])\n",
    "        left_ax.imshow(left_strip, aspect='auto', cmap=token_cmap)\n",
    "        left_ax.axis('off')\n",
    "\n",
    "        # Bottom strip\n",
    "        bottom_ax = fig.add_axes([\n",
    "            pos.x0,\n",
    "            pos.y0 - bottom_bar_height - 0.01,\n",
    "            pos.width,\n",
    "            bottom_bar_height\n",
    "        ])\n",
    "        bottom_ax.imshow(bottom_strip, aspect='auto', cmap=token_cmap)\n",
    "        bottom_ax.axis('off')\n",
    "        \n",
    "        # 5) Corner patch\n",
    "        #   Fill the small square corner at bottom-left so there is no gap.\n",
    "        corner_ax = fig.add_axes([\n",
    "            pos.x0 - left_bar_width,           # left strip's x\n",
    "            pos.y0 - bottom_bar_height,        # bottom strip's y\n",
    "            left_bar_width,                    # same width as left strip\n",
    "            bottom_bar_height                  # same height as bottom strip\n",
    "        ])\n",
    "        # Decide what color to fill the corner with:\n",
    "        # Typically, if your text region covers index=0, we fill with text (blue).\n",
    "        # If your vision region starts at 0, you might want red. \n",
    "        # For simplicity, let's assume text color:\n",
    "        corner_ax.set_facecolor('blue')\n",
    "        corner_ax.axis('off')\n",
    "        \n",
    "        # Legend: Blue patch for text, red patch for vision\n",
    "        blue_patch = mpatches.Patch(color='lightblue', label='Text tokens (System+Instruction)')\n",
    "        red_patch  = mpatches.Patch(color='lightsalmon',  label='Vision tokens')\n",
    "        fig.legend(\n",
    "            handles=[blue_patch, red_patch],\n",
    "            loc='lower center',\n",
    "            ncol=2,\n",
    "            fontsize=12,\n",
    "            bbox_to_anchor=(0.5, 0.02)\n",
    "        )\n",
    "        # Adjust bottom margin to fit the legend\n",
    "        fig.subplots_adjust(bottom=0.11)\n",
    "    \n",
    "    # Increase title & colorbar font sizes\n",
    "    ax.set_title(title, fontsize=16)\n",
    "    cbar = hm.collections[0].colorbar\n",
    "    cbar.ax.set_ylabel('Attention Score', fontsize=14)\n",
    "    cbar.ax.tick_params(labelsize=12)\n",
    "\n",
    "    plt.savefig(output_path, bbox_inches='tight')\n",
    "    if show_image:\n",
    "        plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Gather top 10 attentions for each row\n",
    "    top_ten_attentions = []\n",
    "    for row in averaged_attention:\n",
    "        top_vals, top_idx = torch.topk(row, 10)\n",
    "        top_line = list(zip(top_idx.tolist(), top_vals.tolist()))\n",
    "        top_ten_attentions.append(top_line)\n",
    "    \n",
    "    return top_ten_attentions, averaged_attention\n",
    "\n",
    "# Example usage:\n",
    "# multihead_attention = ...  # shape (1, num_heads, n_tokens, n_tokens)\n",
    "# vision_idx = {'start': 500, 'end': 600}\n",
    "# visualize_attention(multihead_attention, output_path=\"atten_map_1.png\",\n",
    "#                     title=\"Layer 5\", vision_idx=vision_idx, show_image=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multihead_attention shape: torch.Size([1, 28, 1202, 1202])\n",
      "multihead_attention shape: torch.Size([1, 28, 1202, 1202])\n",
      "multihead_attention shape: torch.Size([1, 28, 1202, 1202])\n",
      "multihead_attention shape: torch.Size([1, 28, 1202, 1202])\n",
      "multihead_attention shape: torch.Size([1, 28, 1202, 1202])\n",
      "multihead_attention shape: torch.Size([1, 28, 1202, 1202])\n",
      "multihead_attention shape: torch.Size([1, 28, 1202, 1202])\n",
      "multihead_attention shape: torch.Size([1, 28, 1202, 1202])\n",
      "multihead_attention shape: torch.Size([1, 28, 1202, 1202])\n",
      "multihead_attention shape: torch.Size([1, 28, 1202, 1202])\n",
      "multihead_attention shape: torch.Size([1, 28, 1202, 1202])\n",
      "multihead_attention shape: torch.Size([1, 28, 1202, 1202])\n",
      "multihead_attention shape: torch.Size([1, 28, 1202, 1202])\n",
      "multihead_attention shape: torch.Size([1, 28, 1202, 1202])\n",
      "multihead_attention shape: torch.Size([1, 28, 1202, 1202])\n",
      "multihead_attention shape: torch.Size([1, 28, 1202, 1202])\n",
      "multihead_attention shape: torch.Size([1, 28, 1202, 1202])\n",
      "multihead_attention shape: torch.Size([1, 28, 1202, 1202])\n",
      "multihead_attention shape: torch.Size([1, 28, 1202, 1202])\n",
      "multihead_attention shape: torch.Size([1, 28, 1202, 1202])\n",
      "multihead_attention shape: torch.Size([1, 28, 1202, 1202])\n",
      "multihead_attention shape: torch.Size([1, 28, 1202, 1202])\n",
      "multihead_attention shape: torch.Size([1, 28, 1202, 1202])\n",
      "multihead_attention shape: torch.Size([1, 28, 1202, 1202])\n",
      "multihead_attention shape: torch.Size([1, 28, 1202, 1202])\n",
      "multihead_attention shape: torch.Size([1, 28, 1202, 1202])\n",
      "multihead_attention shape: torch.Size([1, 28, 1202, 1202])\n",
      "multihead_attention shape: torch.Size([1, 28, 1202, 1202])\n"
     ]
    }
   ],
   "source": [
    "for i, attention in enumerate(attention_outputs[0]):\n",
    "    # print(i, attention)\n",
    "    # print(top5_attention)\n",
    "    top5_attention,average_attentions = visualize_attention(attention.cpu(), output_path=\"./attn_maps/atten_map_\"+str(i+1)+\".png\",title=\"Layer \"+str(i+1), vision_idx=vision_idx, show_image=False)\n",
    "    # if i == 0:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global (All attention in 28 layers in one picture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_global_attention(attention_list, output_path=\"global_atten.png\", \n",
    "                               pool_stride=20, vision_idx=None, show_image=False):\n",
    "    \"\"\"\n",
    "    Plots all attention maps (one per layer) in one figure without any tick marks.\n",
    "    \n",
    "    Parameters:\n",
    "      attention_list (list[torch.Tensor]): A list of attention tensors (each with shape: (1, num_heads, n_tokens, n_tokens)).\n",
    "      output_path (str): Path to save the resulting global image.\n",
    "      pool_stride (int): Stride value for average pooling.\n",
    "      vision_idx (dict or None): Optional dictionary with 'start' and 'end' keys to highlight vision tokens.\n",
    "      show_image (bool): Whether to call plt.show() to display the image.\n",
    "      \n",
    "    Returns:\n",
    "      None (the global figure is saved to output_path).\n",
    "    \"\"\"\n",
    "\n",
    "    import torch\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.colors import LogNorm\n",
    "\n",
    "    n_layers = len(attention_list)\n",
    "    # Define grid dimensions; for 28 layers, 7 rows x 4 columns works nicely.\n",
    "    ncols = 4\n",
    "    nrows = (n_layers + ncols - 1) // ncols  # rounds up if not a perfect fit\n",
    "\n",
    "    # Create the figure and axes.\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, \n",
    "                             figsize=(4*ncols, 4*nrows), dpi=200)\n",
    "    # Flatten axes array to simplify iteration.\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Define colormap and normalization (same settings as before).\n",
    "    cmap = plt.get_cmap(\"inferno\")\n",
    "    log_norm = LogNorm(vmin=0.0007, vmax=0.1)\n",
    "\n",
    "    # Process and plot each layer's attention.\n",
    "    for i, multihead_attention in enumerate(attention_list):\n",
    "        ax = axes[i]\n",
    "        # Average over heads. Expecting shape: (1, num_heads, n_tokens, n_tokens)\n",
    "        averaged_attention = torch.mean(multihead_attention, axis=1)[0].float()\n",
    "\n",
    "        # Apply average pooling (note: avg_pool2d requires a 4D tensor).\n",
    "        averaged_attention = torch.nn.functional.avg_pool2d(\n",
    "            averaged_attention.unsqueeze(0).unsqueeze(0),\n",
    "            pool_stride,\n",
    "            stride=pool_stride\n",
    "        ).squeeze(0).squeeze(0)\n",
    "\n",
    "        # Plot heatmap on the specified axis without an individual colorbar.\n",
    "        sns.heatmap(\n",
    "            averaged_attention,\n",
    "            ax=ax,\n",
    "            cmap=cmap,\n",
    "            norm=log_norm,\n",
    "            cbar=False\n",
    "        )\n",
    "\n",
    "        # Remove all tick marks.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "        # Set a title for each subplot.\n",
    "        ax.set_title(f\"Layer {i+1}\", fontsize=8)\n",
    "\n",
    "    # Remove any empty subplots if n_layers does not exactly fill the grid.\n",
    "    for j in range(n_layers, nrows * ncols):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    # Add a single global colorbar on the right side.\n",
    "    fig.subplots_adjust(right=0.85)\n",
    "    cbar_ax = fig.add_axes([0.9, 0.15, 0.03, 0.7])\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=log_norm)\n",
    "    sm.set_array([])\n",
    "    fig.colorbar(sm, cax=cbar_ax, label='Attention score')\n",
    "\n",
    "    # Instead of using tight_layout (which can give warnings when using extra axes),\n",
    "    # we manually adjust the subplot layout.\n",
    "    # plt.tight_layout(rect=[0, 0, 0.9, 1])\n",
    "\n",
    "    # Save and optionally display the figure.\n",
    "    plt.savefig(output_path, bbox_inches='tight')\n",
    "    if show_image:\n",
    "        plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_output_cpu = [attention_outputs[0][i].cpu() for i in range(28)]\n",
    "visualize_global_attention(attention_output_cpu, output_path=\"./attn_maps/global_atten_map.png\", vision_idx=vision_idx, show_image=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen2vl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
