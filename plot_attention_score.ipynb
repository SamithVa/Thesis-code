{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb;\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import torch\n",
    "import PIL.Image as Image\n",
    "\n",
    "from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor\n",
    "# torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "# torch.backends.cuda.enable_flash_sdp(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resize image using PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# img = Image.open('chrome.png', 'r')\n",
    "# w, h = img.size \n",
    "# ratio = w / h\n",
    "# new_h = 720\n",
    "# new_w = round(720 * ratio)\n",
    "# print(new_w, new_h, new_w * new_h // 28 // 28)\n",
    "# img_resize = img.resize((new_w, new_h))\n",
    "# img_resize.save('chrome_resized.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from qwen_vl_utils import process_vision_info\n",
    "# from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor\n",
    "\n",
    "# from model.showui.modeling_showui import ShowUIForConditionalGeneration\n",
    "# from model.showui.processing_showui import ShowUIProcessor\n",
    "\n",
    "### ShowUI Preprocessor\n",
    "# 0. Common setups\n",
    "min_pixels = 256*28*28\n",
    "max_pixels = 1344*28*28\n",
    "# 1. Screenshot -> Graph\n",
    "uigraph_train = True        # Enable ui graph during training\n",
    "uigraph_test = True         # Enable ui graph during inference\n",
    "uigraph_diff = 1            # Pixel difference used for constructing ui graph\n",
    "uigraph_rand = False        # Enable random graph construction \n",
    "# 2. Graph -> Mask \n",
    "uimask_pre = True           # Prebuild patch selection mask in the preprocessor (not in model layers) for efficiency\n",
    "uimask_ratio = 0.5          # Specify the percentage of patch tokens to skip per component\n",
    "uimask_rand = False         # Enable random token selection instead of uniform selection\n",
    "\n",
    "### ShowUI Model\n",
    "lm_skip_ratio = uimask_ratio # valid if not uimask_pre\n",
    "lm_skip_layer = \"[1,28,1]\"   # [1,28,1] means we apply UI guide token selection from 1-th to 28-th layer (28 is the last layer of Qwen2-VL)\n",
    "model_path = \"/data/data1/syc/intern/wanshan/models/Qwen2-VL-7B-Instruct\"\n",
    "\n",
    "processor = Qwen2VLProcessor.from_pretrained(\n",
    "    model_path, \n",
    "    min_pixels=min_pixels, max_pixels=max_pixels,\n",
    "    # uigraph_train=uigraph_train, uigraph_test=uigraph_test, uigraph_diff=uigraph_diff, uigraph_rand=uigraph_rand,\n",
    "    # uimask_pre=True, uimask_ratio=uimask_ratio, uimask_rand=uimask_rand,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eager\n"
     ]
    }
   ],
   "source": [
    "from transformers import Qwen2VLConfig\n",
    "config = Qwen2VLConfig.from_pretrained(model_path)\n",
    "print(config._attn_implementation) # only eager attention implementation can output attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5289796e51fd43a4ba8b2497b063f555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lm_qwen_layer = 28 # LLM Decoder layers\n",
    "device = 'cuda'\n",
    "# def parse_layer_type(str_ranges, L=lm_qwen_layer, default=0):\n",
    "#     # 0 is without layer token selection, 1 is with layer token selection. Below we provide examples:\n",
    "#     # [1,28,1] means that all LM layers use token selection; [1,28,0] means that do not.\n",
    "#     # Interleaved layer-wise '[2,2,1],[4,4,1],[6,6,1],[8,8,1],[10,10,1],[12,12,1],[14,14,1],[16,16,1],[18,18,1],[20,20,1],[22,22,1],[24,24,1],[26,26,1]'\n",
    "#     result = [default] * L\n",
    "#     matches = re.findall(r'\\[\\s*(\\d+)\\s*,\\s*(\\d+)\\s*,\\s*(\\d+)\\s*\\]', str_ranges)\n",
    "#     for start, end, value in matches:\n",
    "#         start, end, value = int(start) - 1, int(end) - 1, int(value)\n",
    "#         if end >= L:\n",
    "#             end = L - 1\n",
    "#         result[start:end + 1] = [value] * (end - start + 1)\n",
    "#     return result\n",
    "\n",
    "# lm_skip_layer = parse_layer_type(lm_skip_layer, 28)\n",
    "# print(lm_skip_layer)\n",
    "\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=device,\n",
    "    # lm_skip_ratio=lm_skip_ratio, lm_skip_layer=lm_skip_layer,\n",
    "    attn_implementation=\"eager\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Please describe this image in detail. Explain what is depicted, including all visible components, structures, or elements, and how they interact with each other. Clarify the purpose of this object or system and the context in which it is typically used. Additionally, explain why this is important — what problems it helps solve, what needs it fulfills, or what advantages it provides. If applicable, describe the technology behind it, its functionality, and its role in broader applications. Your response should be approximately 100 words, providing a comprehensive and informative explanation for better understanding.<|vision_start|><|image_pad|><|vision_end|><|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "img_url = 'chrome_resized.png'\n",
    "vis_dir = 'visualize_imgs'\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Please describe this image in detail. Explain what is depicted, including all visible components, structures, or elements, \"\n",
    "            \"and how they interact with each other. Clarify the purpose of this object or system and the context in which it is typically used. \"\n",
    "            \"Additionally, explain why this is important — what problems it helps solve, what needs it fulfills, or what advantages it provides. \"\n",
    "            \"If applicable, describe the technology behind it, its functionality, and its role in broader applications. \"\n",
    "            \"Your response should be approximately 100 words, providing a comprehensive and informative explanation for better understanding.\"},\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": img_url,\n",
    "                \"min_pixels\": min_pixels,\n",
    "                \"max_pixels\": max_pixels,\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True,\n",
    ")\n",
    "print(text)\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "inputs = inputs.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([1, 1202])\n",
      "attention_mask torch.Size([1, 1202])\n",
      "pixel_values torch.Size([4264, 1176])\n",
      "image_grid_thw torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "for key in inputs.keys():\n",
    "    print(key, inputs[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(inputs[\"select_mask\"].sum(), inputs['select_mask'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151653"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.vision_end_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<vision_state> at 129\n",
      "<vision_end> at 1196\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'start': 130, 'end': 1196}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text_inputs['patch_pos'] = torch.zeros_like(text_inputs['input_ids']) -1\n",
    "vision_idx = {\n",
    "    'start': 0,\n",
    "    'end': 0\n",
    "}\n",
    "\n",
    "for i in range(len(inputs['input_ids'][0])):\n",
    "    # assume here is 1 x L\n",
    "    if inputs['input_ids'][0, i] == config.vision_start_token_id:  \n",
    "        vision_idx['start'] = i+1\n",
    "        print(f'<vision_state> at {i}') # vision_start, image_tokens ... , vision_end\n",
    "    if inputs['input_ids'][0, i] == config.vision_end_token_id:  \n",
    "        print(f'<vision_end> at {i}')\n",
    "        vision_end = i\n",
    "        vision_idx['end'] = i\n",
    "\n",
    "vision_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/syc/anaconda3/envs/qwen2vl/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.01` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/syc/anaconda3/envs/qwen2vl/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.001` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/syc/anaconda3/envs/qwen2vl/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:651: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image depicts the Google Chrome web browser interface. The browser window is open, displaying the Google search homepage. The search bar is prominently displayed in the center, with the Google logo above it. Below the search bar, there are options to add shortcuts for Baidu and other a button to add a new shortcut. The browser also shows various tabs and bookmarks on the top right corner, including Gmail, Images, and All Bookmarks. The bottom of the screen shows the taskbar with various icons, including the Google Chrome icon. The system bar at the bottom of the screen shows the current time and date, as well as the system bar\n",
      "sequences <class 'torch.Tensor'>\n",
      "scores <class 'tuple'>\n",
      "attentions <class 'tuple'>\n",
      "past_key_values <class 'transformers.cache_utils.DynamicCache'>\n"
     ]
    }
   ],
   "source": [
    "generated_ids = model.generate(\n",
    "                    **inputs,\n",
    "                    do_sample=False,\n",
    "                    max_new_tokens=128,\n",
    "                    use_cache=True,\n",
    "                    # stopping_criteria=[stopping_criteria],\n",
    "                    output_attentions=True,\n",
    "                    output_scores=True,\n",
    "                    return_dict_in_generate=True,\n",
    "                    )\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids['sequences'])\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, \n",
    ")[0]\n",
    "print(output_text)\n",
    "for key in generated_ids.keys():\n",
    "    print(key, type(generated_ids[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1202])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1330])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_seqs = generated_ids['sequences']\n",
    "output_seqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "attention_outputs = generated_ids['attentions']\n",
    "print(len(attention_outputs)) # [seq_len, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(attention_outputs[0]) # 28 decoder layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 1202, 1202])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_outputs[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 1202, 1202])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_outputs[0][27].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 1, 1203])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_outputs[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "scores = generated_ids['scores']\n",
    "print(len(scores[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 152064])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# # Suppose:\n",
    "# # C = number of input tokens (e.g., 516)\n",
    "# # G = number of generated tokens you want to combine (64)\n",
    "# C = 516\n",
    "# G = 64\n",
    "# final_seq_len = C + G\n",
    "\n",
    "# # full_attention_outputs will be a list with one tensor per layer (28 in total)\n",
    "# full_attention_outputs = []  # one per decoder layer\n",
    "\n",
    "# # Loop over each layer (assuming 28 layers)\n",
    "# for layer in range(1):\n",
    "#     # Get the cached attention for the input tokens.\n",
    "#     # This tensor has shape [1, 12, C, C].\n",
    "#     cached_attn = attention_outputs[0][layer]   # shape: [1, 12, C, C]\n",
    "#     print(cached_attn.shape)\n",
    "    \n",
    "#     # This will store the new token rows (one per generation step).\n",
    "#     new_tokens_attn = []\n",
    "    \n",
    "#     # For generation steps 1 to G (i.e., the first 64 generated tokens)\n",
    "#     for gen_idx in range(1, G + 1):\n",
    "#         # Get the new token's attention for the layer.\n",
    "#         # It should have shape [1, 12, 1, (C + gen_idx)].\n",
    "#         new_attn = attention_outputs[gen_idx][layer]\n",
    "#         print(new_attn.shape)\n",
    "#         cached_attn = torch.cat((cached_attn,new_tokens_attn), dim=2)\n",
    "#         print(cached_attn.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences\n",
      "scores\n",
      "attentions\n",
      "past_key_values\n"
     ]
    }
   ],
   "source": [
    "for key in generated_ids.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': 130, 'end': 1196}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vision_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1330])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([151644,   8948,    198,  ...,    279,   1849,   3619], device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib.colors import LogNorm, ListedColormap\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "def visualize_attention(multihead_attention, \n",
    "                        output_path=\"atten_map_1.png\",\n",
    "                        title=\"Layer 5\", \n",
    "                        vision_idx=None, \n",
    "                        show_image=False,\n",
    "                        pool_stride=20, \n",
    "                        dpi=200):\n",
    "    \"\"\"\n",
    "    Visualize a single attention map without tick labels on the main heatmap.\n",
    "    Adds thin colored strips along the LEFT and BOTTOM edges to indicate token regions:\n",
    "      - Blue: Text tokens (System+Instruction) outside vision token range\n",
    "      - Red: Vision tokens (vision_idx['start'] to vision_idx['end'])\n",
    "    Additionally, a small corner patch is placed at the bottom-left corner \n",
    "    so there is no extra unfilled square.\n",
    "\n",
    "    Parameters:\n",
    "      multihead_attention (torch.Tensor): shape (1, num_heads, n_tokens, n_tokens).\n",
    "      output_path (str): File path to save the figure.\n",
    "      title (str): Plot title.\n",
    "      vision_idx (dict or None): Dictionary with 'start' and 'end' for vision tokens.\n",
    "      show_image (bool): If True, display the plot.\n",
    "      pool_stride (int): Stride used for average pooling.\n",
    "      dpi (int): Dots per inch for the figure.\n",
    "    \n",
    "    Returns:\n",
    "      top_ten_attentions (list): A list with top 10 (index, score) pairs for each row.\n",
    "      averaged_attention (torch.Tensor): The 2D pooled attention map.\n",
    "    \"\"\"\n",
    "    print(\"multihead_attention shape:\", multihead_attention.shape)\n",
    "    \n",
    "    # 1) Average attention scores over heads\n",
    "    averaged_attention = torch.mean(multihead_attention, axis=1)[0].float()\n",
    "\n",
    "    # 2) Downsample via average pooling\n",
    "    averaged_attention = torch.nn.functional.avg_pool2d(\n",
    "        averaged_attention.unsqueeze(0).unsqueeze(0), \n",
    "        kernel_size=pool_stride, \n",
    "        stride=pool_stride\n",
    "    ).squeeze(0).squeeze(0)\n",
    "\n",
    "    # Number of pooled tokens (square shape)\n",
    "    n_cells = averaged_attention.shape[0]\n",
    "\n",
    "    # 3) Create figure & main heatmap axis (no tick labels)\n",
    "    fig, ax = plt.subplots(figsize=(10, 10), dpi=dpi)\n",
    "    \n",
    "    cmap = plt.get_cmap(\"inferno\")\n",
    "    log_norm = LogNorm(vmin=0.0007, vmax=0.1)\n",
    "    \n",
    "    hm = sns.heatmap(\n",
    "        averaged_attention,\n",
    "        cmap=cmap,\n",
    "        norm=log_norm,\n",
    "        cbar_kws={'label': 'Attention score'},\n",
    "        ax=ax,\n",
    "        xticklabels=False,\n",
    "        yticklabels=False\n",
    "    )\n",
    "    \n",
    "    if vision_idx is not None:\n",
    "        # Convert original token indices → pooled indices\n",
    "        pooled_v_start = vision_idx['start'] // pool_stride\n",
    "        pooled_v_end   = vision_idx['end']   // pool_stride\n",
    "        \n",
    "        # 0 => text token, 1 => vision token\n",
    "        token_types = np.zeros(n_cells, dtype=int)\n",
    "        token_types[pooled_v_start:pooled_v_end] = 1\n",
    "        \n",
    "        # For the strips\n",
    "        left_strip   = token_types.reshape(n_cells, 1)  # vertical\n",
    "        bottom_strip = token_types.reshape(1, n_cells)  # horizontal\n",
    "\n",
    "        # Colormap for the strips: 0=blue, 1=red\n",
    "        token_cmap = ListedColormap(['lightblue', 'lightsalmon'])\n",
    "        \n",
    "        # 4) Position these strips flush with the heatmap\n",
    "        pos = ax.get_position()\n",
    "\n",
    "        # Thinner bars\n",
    "        left_bar_width    = 0.02\n",
    "        bottom_bar_height = 0.02\n",
    "\n",
    "        # Left strip\n",
    "        left_ax = fig.add_axes([\n",
    "            pos.x0 - left_bar_width - 0.01,\n",
    "            pos.y0,\n",
    "            left_bar_width ,\n",
    "            pos.height\n",
    "        ])\n",
    "        left_ax.imshow(left_strip, aspect='auto', cmap=token_cmap)\n",
    "        left_ax.axis('off')\n",
    "\n",
    "        # Bottom strip\n",
    "        bottom_ax = fig.add_axes([\n",
    "            pos.x0,\n",
    "            pos.y0 - bottom_bar_height - 0.01,\n",
    "            pos.width,\n",
    "            bottom_bar_height\n",
    "        ])\n",
    "        bottom_ax.imshow(bottom_strip, aspect='auto', cmap=token_cmap)\n",
    "        bottom_ax.axis('off')\n",
    "        \n",
    "        # 5) Corner patch\n",
    "        #   Fill the small square corner at bottom-left so there is no gap.\n",
    "        corner_ax = fig.add_axes([\n",
    "            pos.x0 - left_bar_width,           # left strip's x\n",
    "            pos.y0 - bottom_bar_height,        # bottom strip's y\n",
    "            left_bar_width,                    # same width as left strip\n",
    "            bottom_bar_height                  # same height as bottom strip\n",
    "        ])\n",
    "        # Decide what color to fill the corner with:\n",
    "        # Typically, if your text region covers index=0, we fill with text (blue).\n",
    "        # If your vision region starts at 0, you might want red. \n",
    "        # For simplicity, let's assume text color:\n",
    "        corner_ax.set_facecolor('blue')\n",
    "        corner_ax.axis('off')\n",
    "        \n",
    "        # Legend: Blue patch for text, red patch for vision\n",
    "        blue_patch = mpatches.Patch(color='lightblue', label='Text tokens (System+Instruction)')\n",
    "        red_patch  = mpatches.Patch(color='lightsalmon',  label='Vision tokens')\n",
    "        fig.legend(\n",
    "            handles=[blue_patch, red_patch],\n",
    "            loc='lower center',\n",
    "            ncol=2,\n",
    "            fontsize=12,\n",
    "            bbox_to_anchor=(0.5, 0.02)\n",
    "        )\n",
    "        # Adjust bottom margin to fit the legend\n",
    "        fig.subplots_adjust(bottom=0.11)\n",
    "    \n",
    "    # Increase title & colorbar font sizes\n",
    "    ax.set_title(title, fontsize=16)\n",
    "    cbar = hm.collections[0].colorbar\n",
    "    cbar.ax.set_ylabel('Attention Score', fontsize=14)\n",
    "    cbar.ax.tick_params(labelsize=12)\n",
    "\n",
    "    plt.savefig(output_path, bbox_inches='tight')\n",
    "    if show_image:\n",
    "        plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Gather top 10 attentions for each row\n",
    "    top_ten_attentions = []\n",
    "    for row in averaged_attention:\n",
    "        top_vals, top_idx = torch.topk(row, 10)\n",
    "        top_line = list(zip(top_idx.tolist(), top_vals.tolist()))\n",
    "        top_ten_attentions.append(top_line)\n",
    "    \n",
    "    return top_ten_attentions, averaged_attention\n",
    "\n",
    "# Example usage:\n",
    "# multihead_attention = ...  # shape (1, num_heads, n_tokens, n_tokens)\n",
    "# vision_idx = {'start': 500, 'end': 600}\n",
    "# visualize_attention(multihead_attention, output_path=\"atten_map_1.png\",\n",
    "#                     title=\"Layer 5\", vision_idx=vision_idx, show_image=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multihead_attention shape: torch.Size([1, 28, 1202, 1202])\n",
      "multihead_attention shape: torch.Size([1, 28, 1202, 1202])\n",
      "multihead_attention shape: torch.Size([1, 28, 1202, 1202])\n",
      "multihead_attention shape: torch.Size([1, 28, 1202, 1202])\n",
      "multihead_attention shape: torch.Size([1, 28, 1202, 1202])\n",
      "multihead_attention shape: torch.Size([1, 28, 1202, 1202])\n",
      "multihead_attention shape: torch.Size([1, 28, 1202, 1202])\n",
      "multihead_attention shape: torch.Size([1, 28, 1202, 1202])\n",
      "multihead_attention shape: torch.Size([1, 28, 1202, 1202])\n",
      "multihead_attention shape: torch.Size([1, 28, 1202, 1202])\n",
      "multihead_attention shape: torch.Size([1, 28, 1202, 1202])\n",
      "multihead_attention shape: torch.Size([1, 28, 1202, 1202])\n",
      "multihead_attention shape: torch.Size([1, 28, 1202, 1202])\n",
      "multihead_attention shape: torch.Size([1, 28, 1202, 1202])\n",
      "multihead_attention shape: torch.Size([1, 28, 1202, 1202])\n",
      "multihead_attention shape: torch.Size([1, 28, 1202, 1202])\n",
      "multihead_attention shape: torch.Size([1, 28, 1202, 1202])\n",
      "multihead_attention shape: torch.Size([1, 28, 1202, 1202])\n",
      "multihead_attention shape: torch.Size([1, 28, 1202, 1202])\n",
      "multihead_attention shape: torch.Size([1, 28, 1202, 1202])\n",
      "multihead_attention shape: torch.Size([1, 28, 1202, 1202])\n",
      "multihead_attention shape: torch.Size([1, 28, 1202, 1202])\n",
      "multihead_attention shape: torch.Size([1, 28, 1202, 1202])\n",
      "multihead_attention shape: torch.Size([1, 28, 1202, 1202])\n",
      "multihead_attention shape: torch.Size([1, 28, 1202, 1202])\n",
      "multihead_attention shape: torch.Size([1, 28, 1202, 1202])\n",
      "multihead_attention shape: torch.Size([1, 28, 1202, 1202])\n",
      "multihead_attention shape: torch.Size([1, 28, 1202, 1202])\n"
     ]
    }
   ],
   "source": [
    "for i, attention in enumerate(attention_outputs[0]):\n",
    "    # print(i, attention)\n",
    "    # print(top5_attention)\n",
    "    top5_attention,average_attentions = visualize_attention(attention.cpu(), output_path=\"./attn_maps/atten_map_\"+str(i+1)+\".png\",title=\"Layer \"+str(i+1), vision_idx=vision_idx, show_image=False)\n",
    "    # if i == 0:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global (All attention in 28 layers in one picture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_global_attention(attention_list, output_path=\"global_atten.png\", \n",
    "                               pool_stride=20, vision_idx=None, show_image=False):\n",
    "    \"\"\"\n",
    "    Plots all attention maps (one per layer) in one figure without any tick marks.\n",
    "    \n",
    "    Parameters:\n",
    "      attention_list (list[torch.Tensor]): A list of attention tensors (each with shape: (1, num_heads, n_tokens, n_tokens)).\n",
    "      output_path (str): Path to save the resulting global image.\n",
    "      pool_stride (int): Stride value for average pooling.\n",
    "      vision_idx (dict or None): Optional dictionary with 'start' and 'end' keys to highlight vision tokens.\n",
    "      show_image (bool): Whether to call plt.show() to display the image.\n",
    "      \n",
    "    Returns:\n",
    "      None (the global figure is saved to output_path).\n",
    "    \"\"\"\n",
    "\n",
    "    import torch\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.colors import LogNorm\n",
    "\n",
    "    n_layers = len(attention_list)\n",
    "    # Define grid dimensions; for 28 layers, 7 rows x 4 columns works nicely.\n",
    "    ncols = 4\n",
    "    nrows = (n_layers + ncols - 1) // ncols  # rounds up if not a perfect fit\n",
    "\n",
    "    # Create the figure and axes.\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, \n",
    "                             figsize=(4*ncols, 4*nrows), dpi=200)\n",
    "    # Flatten axes array to simplify iteration.\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Define colormap and normalization (same settings as before).\n",
    "    cmap = plt.get_cmap(\"inferno\")\n",
    "    log_norm = LogNorm(vmin=0.0007, vmax=0.1)\n",
    "\n",
    "    # Process and plot each layer's attention.\n",
    "    for i, multihead_attention in enumerate(attention_list):\n",
    "        ax = axes[i]\n",
    "        # Average over heads. Expecting shape: (1, num_heads, n_tokens, n_tokens)\n",
    "        averaged_attention = torch.mean(multihead_attention, axis=1)[0].float()\n",
    "\n",
    "        # Apply average pooling (note: avg_pool2d requires a 4D tensor).\n",
    "        averaged_attention = torch.nn.functional.avg_pool2d(\n",
    "            averaged_attention.unsqueeze(0).unsqueeze(0),\n",
    "            pool_stride,\n",
    "            stride=pool_stride\n",
    "        ).squeeze(0).squeeze(0)\n",
    "\n",
    "        # Plot heatmap on the specified axis without an individual colorbar.\n",
    "        sns.heatmap(\n",
    "            averaged_attention,\n",
    "            ax=ax,\n",
    "            cmap=cmap,\n",
    "            norm=log_norm,\n",
    "            cbar=False\n",
    "        )\n",
    "\n",
    "        # Remove all tick marks.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "        # Set a title for each subplot.\n",
    "        ax.set_title(f\"Layer {i+1}\", fontsize=8)\n",
    "\n",
    "    # Remove any empty subplots if n_layers does not exactly fill the grid.\n",
    "    for j in range(n_layers, nrows * ncols):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    # Add a single global colorbar on the right side.\n",
    "    fig.subplots_adjust(right=0.85)\n",
    "    cbar_ax = fig.add_axes([0.9, 0.15, 0.03, 0.7])\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=log_norm)\n",
    "    sm.set_array([])\n",
    "    fig.colorbar(sm, cax=cbar_ax, label='Attention score')\n",
    "\n",
    "    # Instead of using tight_layout (which can give warnings when using extra axes),\n",
    "    # we manually adjust the subplot layout.\n",
    "    # plt.tight_layout(rect=[0, 0, 0.9, 1])\n",
    "\n",
    "    # Save and optionally display the figure.\n",
    "    plt.savefig(output_path, bbox_inches='tight')\n",
    "    if show_image:\n",
    "        plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_output_cpu = [attention_outputs[0][i].cpu() for i in range(28)]\n",
    "visualize_global_attention(attention_output_cpu, output_path=\"./attn_maps/global_atten_map.png\", vision_idx=vision_idx, show_image=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen2vl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
