{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Image\n",
    "import os\n",
    "from transformers import Qwen2VLProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import ast  # To safely evaluate JSON-like strings\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "def draw_click_point(\n",
    "    img_path,\n",
    "    qwen2vl_x, qwen2vl_y,\n",
    "    seeclick_x, seeclick_y,\n",
    "    bbox,\n",
    "    instruction,\n",
    "    gap_between_text_and_image: int = 15\n",
    "):\n",
    "    \"\"\"\n",
    "    Draw instruction text in a white banner at the top, then leave\n",
    "    `gap_between_text_and_image` pixels of blank, then the image.\n",
    "    \"\"\"\n",
    "    # 1) Load image\n",
    "    image = Image.open(img_path)\n",
    "    width, height = image.size\n",
    "\n",
    "    # 2) Choose a Chinese‐capable font (fallback to default)\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"/usr/share/fonts/truetype/noto/NotoSansSC-Regular.otf\", size=20)\n",
    "    except IOError:\n",
    "        font = ImageFont.load_default(size=20)\n",
    "\n",
    "    # 3) Measure instruction text\n",
    "    tmp = ImageDraw.Draw(image)\n",
    "    left, top, right, bottom = tmp.textbbox((0,0), instruction, font=font)\n",
    "    text_w, text_h = right - left, bottom - top\n",
    "\n",
    "    # 4) Compute paddings\n",
    "    top_margin = 5\n",
    "    bottom_margin = gap_between_text_and_image\n",
    "    padding_top = text_h + top_margin + bottom_margin\n",
    "\n",
    "    # 5) New canvas: original height + that padding_top\n",
    "    new_img = Image.new(\"RGB\", (width, height + padding_top), \"white\")\n",
    "    new_img.paste(image, (0, padding_top))\n",
    "\n",
    "    draw = ImageDraw.Draw(new_img)\n",
    "\n",
    "    # 6) Draw the instruction centered in its band\n",
    "    x_text = (width - text_w) / 2\n",
    "    y_text = top_margin\n",
    "    draw.text((x_text, y_text), \"Instruction: \" + instruction, fill=\"black\", font=font)\n",
    "\n",
    "    # 7) Convert relative → absolute (accounting for padding)\n",
    "    def to_abs(rx, ry):\n",
    "        return rx * width, padding_top + ry * height\n",
    "\n",
    "    qx, qy = to_abs(qwen2vl_x, qwen2vl_y)\n",
    "    sx, sy = to_abs(seeclick_x, seeclick_y)\n",
    "    bx0, by0 = to_abs(bbox[0], bbox[1])\n",
    "    bx1, by1 = to_abs(bbox[2], bbox[3])\n",
    "\n",
    "    # 8) Draw bbox + circles\n",
    "    draw.rectangle([bx0, by0, bx1, by1], outline=\"green\", width=2)\n",
    "    r = 5\n",
    "    draw.ellipse((qx-r, qy-r, qx+r, qy+r), fill=\"blue\")\n",
    "    draw.ellipse((sx-r, sy-r, sx+r, sy+r), fill=\"red\")\n",
    "\n",
    "    # 9) Return PNG bytes\n",
    "    with io.BytesIO() as out:\n",
    "        new_img.save(out, format=\"PNG\")\n",
    "        return out.getvalue()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON file\n",
    "qwen2vl_pred_path = \"/home/syc/intern/wanshan/Thesis_result/Mind2Web/Text-History/Qwen2-VL-7B_naive/qwen2vl_resampler_7b_keep_1_date_0406_website.json\"  # Change this to your JSON file path\n",
    "# json_file = \"/home/syc/intern/wanshan/Qwen2-VL/agent_tasks/custom_training_script/qwen2vl_train_train.json\"\n",
    "with open(qwen2vl_pred_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "# Filter instances where Ele_match is False\n",
    "qwen2vl_data = [\n",
    "    [step for step in episode] \n",
    "    for episode in data\n",
    "]\n",
    "\n",
    "# Initialize index\n",
    "episode_index = 0\n",
    "step_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeclick_pred_path = \"/home/syc/intern/wanshan/SeeClick/visualize_result/seeclick_mind2map_ckpt_4000/seeclick_mind2web_website.json\"\n",
    "with open(seeclick_pred_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "# Filter instances where Ele_match is False\n",
    "seeclick_data = [\n",
    "    [step for step in episode] \n",
    "    for episode in data\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 7)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(qwen2vl_data[0]), len(seeclick_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.278, 0.64, 0.528, 0.688]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ground_truth_path = \"/home/syc/intern/wanshan/Qwen2-VL/data/subset_100_samples.json\"\n",
    "# with open(ground_truth_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#     ground_truth_data = json.load(f)\n",
    "\n",
    "qwen2vl_data[0][0]['bbox_ref'] # x_left, y_left, x_right, y_right\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "navigation_sequence  = []\n",
    "for episode_idx, episode in enumerate(qwen2vl_data):\n",
    "    for step_idx, step in enumerate(episode):\n",
    "        if step['Ele_match'] == True and seeclick_data[episode_idx][step_idx]['Ele_match'] == False:\n",
    "            navigation_sequence.append((episode_idx, step_idx))\n",
    "sequence_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "444"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(navigation_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_draw_args = None  # will hold (img_path, qx, qy, sx, sy, bbox)\n",
    "\n",
    "def save_image(_):\n",
    "    if last_draw_args is None:\n",
    "        print(\"Nothing to save yet.\")\n",
    "        return\n",
    "    img_bytes = draw_click_point(*last_draw_args)\n",
    "    fname = f\"annotation_step_{sequence_index:03d}.png\"\n",
    "    with open(fname, \"wb\") as f:\n",
    "        f.write(img_bytes)\n",
    "    print(f\"Saved ▶ {fname}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a984f8b225e7469fb088040881d6fd64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=''), HTML(value=''), Image(value=b''), HBox(children=(Button(description='Next Step'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ▶ annotation_step_000.png\n",
      "Saved ▶ annotation_step_001.png\n",
      "Saved ▶ annotation_step_002.png\n",
      "Saved ▶ annotation_step_005.png\n",
      "Saved ▶ annotation_step_007.png\n",
      "Saved ▶ annotation_step_012.png\n",
      "Saved ▶ annotation_step_015.png\n",
      "Saved ▶ annotation_step_016.png\n",
      "Saved ▶ annotation_step_017.png\n",
      "Saved ▶ annotation_step_019.png\n",
      "Saved ▶ annotation_step_020.png\n",
      "Saved ▶ annotation_step_022.png\n",
      "Saved ▶ annotation_step_024.png\n",
      "Saved ▶ annotation_step_025.png\n",
      "Saved ▶ annotation_step_026.png\n",
      "Saved ▶ annotation_step_027.png\n",
      "Saved ▶ annotation_step_029.png\n",
      "Saved ▶ annotation_step_030.png\n",
      "Saved ▶ annotation_step_031.png\n",
      "Saved ▶ annotation_step_033.png\n",
      "Saved ▶ annotation_step_034.png\n",
      "Saved ▶ annotation_step_036.png\n",
      "Saved ▶ annotation_step_037.png\n",
      "Saved ▶ annotation_step_038.png\n",
      "Saved ▶ annotation_step_039.png\n",
      "Saved ▶ annotation_step_040.png\n",
      "Saved ▶ annotation_step_041.png\n",
      "Saved ▶ annotation_step_042.png\n",
      "Saved ▶ annotation_step_044.png\n",
      "Saved ▶ annotation_step_045.png\n",
      "Saved ▶ annotation_step_046.png\n",
      "Saved ▶ annotation_step_047.png\n",
      "Saved ▶ annotation_step_049.png\n",
      "Saved ▶ annotation_step_051.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def update_display():\n",
    "    global sequence_index, last_draw_args\n",
    "    \n",
    "    if sequence_index < len(navigation_sequence):\n",
    "        episode_index, step_index = navigation_sequence[sequence_index]\n",
    "        episode_qwen2vl = qwen2vl_data[episode_index]\n",
    "        episode_seeclick = seeclick_data[episode_index]\n",
    "        # Ensure that the step index is valid for the current episode\n",
    "        if step_index < len(episode_qwen2vl):\n",
    "            step_qwen2vl = episode_qwen2vl[step_index]\n",
    "            step_seeclick = episode_seeclick[step_index]\n",
    "\n",
    "            instruction = step_qwen2vl.get('instruction')\n",
    "\n",
    "            \n",
    "            # Update the text for each prediction\n",
    "            sentence_qwen2vl.value = f\"**Qwen2VL Action(s):** {step_qwen2vl}\"\n",
    "            sentence_seeclick.value = f\"**SeeClick Action(s):** {step_seeclick}\"\n",
    "            \n",
    "            # Extract click coordinates from the sentence JSON data\n",
    "            try:\n",
    "                action_data_qwen2vl = ast.literal_eval(step_qwen2vl[\"sentence\"][0])\n",
    "                qwen2vl_click_x, qwen2vl_click_y = action_data_qwen2vl.get(\"click_point\", (0, 0))\n",
    "                bbox = step_qwen2vl.get(\"bbox_ref\", None)\n",
    "                \n",
    "                action_data_seeclick = ast.literal_eval(step_seeclick[\"sentence\"])\n",
    "                seeclick_click_x, seeclick_click_y = action_data_seeclick.get(\"click_point\", (0, 0))\n",
    "            except Exception as e:\n",
    "                # If extraction fails, default to top-left for both\n",
    "                qwen2vl_click_x, qwen2vl_click_y = 0, 0\n",
    "                seeclick_click_x, seeclick_click_y = 0, 0\n",
    "            \n",
    "            # Load the image from the Qwen2VL step (assuming it's the same image used by SeeClick)\n",
    "            img_path = step_qwen2vl[\"img_path\"]\n",
    "            \n",
    "            if os.path.exists(img_path):\n",
    "                # Draw both click points on the image and update the widget\n",
    "                last_draw_args = (img_path, qwen2vl_click_x, qwen2vl_click_y, seeclick_click_x, seeclick_click_y, bbox, instruction)\n",
    "                png = draw_click_point(*last_draw_args)\n",
    "                image_widget.value = png\n",
    "            else:\n",
    "                sentence_qwen2vl.value += f\"\\n(Error: Image not found at {img_path})\"\n",
    "        else:\n",
    "            sentence_qwen2vl.value = \"No more steps in this episode.\"\n",
    "            image_widget.value = b\"\"\n",
    "    else:\n",
    "        sentence_qwen2vl.value = \"\"\n",
    "        image_widget.value = b\"\"\n",
    "\n",
    "def next_step(_):\n",
    "    global sequence_index\n",
    "    # Move to the next item in the navigation sequence if available\n",
    "    if sequence_index < len(navigation_sequence) - 1:\n",
    "        sequence_index += 1\n",
    "    update_display()\n",
    "\n",
    "# Widgets initialization\n",
    "sentence_qwen2vl = widgets.HTML()\n",
    "sentence_seeclick = widgets.HTML()\n",
    "image_widget = widgets.Image(format='png')  # Single image widget for merged output\n",
    "next_button = widgets.Button(description=\"Next Step\")\n",
    "save_button     = widgets.Button(description=\"Save Image\", button_style=\"success\")\n",
    "next_button.on_click(next_step)\n",
    "save_button.on_click(save_image)\n",
    "# Initial display update and layout\n",
    "display(\n",
    "    widgets.VBox([\n",
    "        sentence_qwen2vl,\n",
    "        sentence_seeclick,\n",
    "        image_widget,\n",
    "        widgets.HBox([next_button, save_button])\n",
    "    ])\n",
    ")\n",
    "update_display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen2vl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
